import numpy as np
import torch
from chonkie import SentenceChunker, SemanticChunker
from siliconflow_embeddings import SiliconFlowEmbeddings

dis_comp = 3
alpha = 2
max_length = 256

def _dynamic_chunking(output_embs, chunk_annotations, threshold=0.1):
        """Dynamic chunking based on the similarity of the embeddings.
        This method is not implemented yet.
        """
        class Span:
            def __init__(self, start, end, prev=None, next=None):
                self.start = start
                self.end = end
                self.prev = prev
                self.next = next
        
        def cal_similarity(embeddings: np.ndarray) -> np.ndarray:
            # Normalize embeddings to unit length to compute cosine similarity.
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            norm_embeddings = embeddings / (norms + 1e-8)
            sim_matrix = np.dot(norm_embeddings, norm_embeddings.T)
            # Set diagonal to 1 (self-similarity)
            np.fill_diagonal(sim_matrix, 1)
            n = sim_matrix.shape[0]
            indices = np.arange(n)
            # Compute the squared difference matrix: element (i,j) = (|i-j|)^2
            if dis_comp == 0:
                denom = (np.abs(indices.reshape(-1, 1) - indices.reshape(1, -1)))**2
            elif dis_comp == 1:
                denom = (np.abs(indices.reshape(-1, 1) - indices.reshape(1, -1)))
            elif dis_comp == 2:
                # The dis is 1 / 1 + (|i - j|)
                denom = 1 / ((np.abs(indices.reshape(-1, 1) - indices.reshape(1, -1))) * alpha)
            else:
                denom = np.ones((n, n))
            # denom = (np.abs(indices.reshape(-1, 1) - indices.reshape(1, -1)))
            # Avoid division by zero on the diagonal (set divisor to 1 for diagonal elements)
            denom[denom == 0] = 1
            return sim_matrix / denom
        
        def cal_inside_sim(start, mid, end):
            # span from [start, mid - 1], [mid, end]
            # Calculate similarity sum for indices [start, mid] (upper triangle, excluding diagonal)
            sim_start_mid = np.triu(sim_matrix[start:mid, start:mid], k=0).sum()
            # Calculate similarity sum for indices [mid+1, end] (upper triangle, excluding diagonal)
            sim_mid_end = np.triu(sim_matrix[mid:end+1, mid:end+1], k=0).sum()
            return sim_start_mid, sim_mid_end
        
        def cal_inside_coefficient(start, mid, end):
            cnt = (mid - start - 1) * (mid - start) / 2
            cnt += (end - mid) * (end - mid + 1) / 2
            return cnt
        
        def cal_outside_sim(start, mid, end):
            if start == mid:
                return 0
            sim_outside = sim_matrix[start:mid, mid:end+1].sum()
            return sim_outside
        
        def cal_outside_coefficient(start, mid, end):
            if start == mid:
                return 1
            return (mid - start) * (end - mid + 1)
        
        def if_span_is_too_big(start, end):
            l = 0
            for i in range(start, end + 1):
                l += chunk_annotations[i][1] - chunk_annotations[i][0]
            if l >= max_length:
                return True
        
        def recursive_merge(span1, span2):
            nonlocal tail
            start1 = span1.start
            end1 = span1.end
            start2 = span2.start
            end2 = span2.end
            split_points = []
            if start1 == end1 and start2 == end2:
                chunk_fits = not(if_span_is_too_big(start1, end1) or if_span_is_too_big(start2, end2))
                if sim_matrix[start1, start2] > threshold and chunk_fits:
                    span1.start = start1
                    span1.end = end2
                    span1.next = span2.next
                    if span2.next is not None:
                        span2.next.prev = span1
                    span2.next = None
                    span2.prev = None
                    del span2
                    if span1.next is None:
                        tail = span1
                    if span1.prev is not root:
                        recursive_merge(span1.prev, span1)
                return
            
            for mid in range(min(start1, start2), max(end1, end2) + 1):
                sim_inside = sum(cal_inside_sim(start1, mid, end2)) / cal_inside_coefficient(start1, mid, end2)
                sim_outside = cal_outside_sim(start1, mid, end2) / cal_outside_coefficient(start1, mid, end2)
                split_points.append([sim_inside - sim_outside, mid, sim_inside, sim_outside])
            
            split_points = sorted(split_points, key=lambda x: x[0], reverse=True)
            
            # Find the best split with chunk_size < max_chunk_size
            best_split = start2
            for split_point in split_points:
                mid = split_point[1]
                if not(if_span_is_too_big(start1, mid - 1) or if_span_is_too_big(mid, end2)):
                    best_split = mid
                    break
            
            # If the split is the same as former, stop recursive split
            if best_split == start2:
                return
            else:
                # If the split merges the two span
                if best_split == start1:
                    span1.start = start1
                    span1.end = end2
                    span1.next = span2.next
                    if span2.next is not None:
                        span2.next.prev = span1
                    span2.prev = None
                    span2.next = None
                    del span2
                    # update tail again for the original tail may be deleted
                    if span1.next is None:
                        tail = span1
                    if span1.prev is not root:
                        recursive_merge(span1.prev, span1)
                    return
                
                # The split changes the former span, recursive process the original splits
                elif best_split != start2:
                    span1.end = best_split - 1
                    span2.start = best_split
                    if span1.prev is not root:
                        recursive_merge(span1.prev, span1)
        
        
        sim_matrix = cal_similarity(output_embs)
        np.fill_diagonal(sim_matrix, 0)
        root = Span(-1, -1)
        tail = root
        for idx, chunk in enumerate(output_embs):
            if idx == 0:
                root.next = Span(0, 0, root, None)
                tail = root.next
            else:
                new_span = Span(idx, idx, tail, None)
                tail.next = new_span
                tail = new_span
                recursive_merge(tail.prev, tail)
        
        # Collect the merged spans
        pooled_embeddings = []
        span = root.next
        chunk_spans =[]
        while span is not None:
            print(f"Span: {span.start} - {span.end}")
            # print(f"text is: {", ".join(chunks[span.start:span.end + 1])}")
            # Compute mean pooled embedding for this span (inclusive of both start and end indices)
            pooled_embed = np.mean(output_embs[span.start:span.end + 1], axis=0)
            pooled_embeddings.append(pooled_embed)
            chunk_spans.append([span.start, span.end])
            span = span.next
        return pooled_embeddings, chunk_spans

from transformers import AutoModel
from transformers import AutoTokenizer

# from chunked_pooling import chunked_pooling, chunk_by_sentences

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# input_text = "Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area."

input_text = '''"北京理工大学硕士学位论文1第1章绪论1.1.研究背景及意义1.1.1.研究背景创意是表演艺术中最为重要和令人兴奋的元素之一。如今的表演形式日益多样化，而表演作品的质量往往取决于背后的创意水平。传统的表演创意过程以导演的经验为主导，很大程度上围绕导演的艺术素养和风格导向展开，但这种方式往往需要耗费大量时间和资金，同时也容易受到导演审美疲劳的影响，导致创意思路受限、创意效果不佳等问题。随着新一代信息技术的发展，数字化方法为解决这些问题带来新思路和新视野上的拓展。计算机技术的应用为表演创意智能化提供了更多可能性和更广阔的前景。为了能够让表演创意过程更高效、创意效果更精彩，人工智能、情感计算等前沿技术被用来跨学科研究。例如：通过对表演中关键创意元素进行数字化描述形成基于表演创意的智能涌现；利用人机交互和情感计算进行基于观众关注度的表演创意评估；通过建立虚拟仿真表演空间来对创意效果进行实景推演等。这些研究利用数字化和智能化的方法在如何产生和改进表演创意的过程中取得了一定的进展，但是面对愈发复杂的表演环境、愈发多样的表演内容以及观众对表演艺术的审美和认知的不断提高，如何提升表演创意生成的效率和质量还是一个十分具有挑战性的问题。因为在面向表演进行创意生成时，需要根据表演主题、传达的情感、观众群体等因素来生成相应的剧本，这就需要对现有的剧本、文化、情感载体等资料进行深入学习。然而单纯的模仿还难以满足要求，因为如何保持创意性是第二大难题，只有具有一定的创造力和想象力，才能生成有趣又独特的内容。这就需要模型既有一定的学习能力和模仿能力，又有创造力，能够自主提出新的内容。除此之外，为了避免生成重复和低质量内容，还需要模型具有一定的判断能力和筛选能力，才能自动区分创意结果的好坏、新旧、有趣无趣等。随着互联网时代信息的爆炸式增长，大量的数据积累为发现和分析现有规律奠定基础，同时也为机器的学习和模仿提供参照物，但如果机器仅仅是在数据规律下进行仿写或续写，会导致完成的“创意”缺乏情感色彩和共情表达能力。由此我们提出了面向规则研究和情感加成的创意生成框架，进行基于情感的表演创意生成。具体包括：基于优秀剧本的中国表演创意建模，基于情感权重的创意资源知识北京理工大学硕士学位论文2库搭建，以及多条件约束的创意生成研究。1.1.2.研究意义近年来，随着我国经济的发展和国力的增强，在国际舞台上承办了多项重大体育赛事，如2008年北京夏季奥运会和2022年北京冬季奥运会等。同时，就国内而言也围绕着一系列标志性历史节点举行了不少庆典和主题演出，如庆祝中国共产党成立100周年文艺演出大型情景史诗“伟大征程”等。这些文艺演出作为庆典活动的主要形式之一，通过精心的编排创作，将我国博大精深且源远流长的文化搬上舞台。文艺表演不仅能够为庆典活动增添节日氛围，更重要的是可以通过舞台叙事展现富有民族意识的象征和符号，使观众更深刻地感受到表演的魅力，增强对身份的认同感和文化自豪感。表演的价值已经逐渐超过了活动自身，而是在为整个社会塑造着一个共同拥有的时代印记。因此举办更具有意义和深度的文艺表演对促进文化传播和价值交流至关重要。受当下数字经济政策导向和互联网交叉领域成果影响，艺术表演领域的智能化研究也成为一大热点，但是面向中国表演创意元素构成的可计算知识体系构建方面稍显空白，我们习惯用理性的思维和通用的元素处理方法去描述事实，这种方式固然可以迁移到创意生成中去，却会导致艺术性质量低，生成创意重复度高等问题。智能创意生成方法真正将计算机智能引入艺术表演中去，突破表演创意体系缺乏、数字化进程慢、智能化程度低的问题，提高艺术创作工作效率，通过学习大量数据和模式来发掘更多的创意可能性，拓展创意边界，提高创意质量，推动人工智能技术在艺术领域的发展，让创意生成更好地服务于人类的生产和生活。目前也已经有很多成功的交叉应用都让我们看到创意智能化的发展和应用前景，但是针对表演创意生成这一领域的研究与开发还略显空白。本文对当前的表演创意生成研究进行分析，发现主要存在以下两个方面的问题：1、智能创意生成方法缺乏创意体系的依据和规则指引。生成依据和创意规则直接影响生成或构建的创意质量，对表演创意的可计算建模和体系研究是创意生成的基础也是不可缺少的关键环节。但是目前的创意生成过程大多分为两类，最主流的还是导演及团队进行基于主观审美经验的创意产生，这种方式导演的经验即是依据与规则，但存在难以宣之于口或完成数字转化的缺陷；计算机技术方面则主要是基于通用语言或图像这类基础素材的学习与生成模型，这种方式虽然带来了智能化程度上的提升，北京理工大学硕士学位论文3但是没有一个明确的创意体系进行指导，只靠机器全流程无监督式的模仿，必然带来创意可用性的下降。因此需要我们用智能化的方法从艺术的角度提炼创意体系规则，转换创意生成模式。2、利用计算机生成的创意难以融入情感要素。在创意产生过程中，人类往往会借助情感和主观体验，产生独特的想法和创意，这也是创意具有人性化特点的表现。例如，一首歌曲中蕴含的情感、一个画作中传递的主题、一个广告中呈现的情境，都是人性化创意的体现。然而，智能化手段主要依靠数据分析和统计规律来进行计算和生成，往往导致生成的创意缺乏情感和人性化的审美。因此需要我们将情感这一最出彩的艺术要素融入创意生成流程，构造情感为导向的生成体系。针对以上的创意生成过程中存在的问题，本文建立了一个基于大量优秀剧本的为研究基础的中国表演创意体系，构建了一个基于情感权重的创意资源知识库，并以此体系和知识库为基础提出了一种基于多个情感实体与属性的多条件约束创意生成方法，让计算机能够在创意体系的指导下进行基于情感的智能创意生成。1.2.国内外现状智能创意生成一直是人工智能领域的研究热点之一，研究人员希望通过人工智能模拟人类大脑思考方式，其研究范围涉及到图像、音频、视频、文本等多个前沿领域。有关智能创意生成方法的历史可以追溯到上个世纪90年代，当时研究人员开始探索计算机如何在艺术设计领域发挥其高效率、高交互性的优势。根据发展情况和主流技术成果可以将其分为三个阶段：上世纪90年代，研究人员尝试将艺术和计算机学科相结合，研究创造性思维和人工智能之间的可能性，于是利用计算机程序辅助音乐和图像等艺术作品产生的研究活动开始兴起。这个时期虽然还处于智能化进入艺术领域发展的初期，但已经开始出现一些较早的创意辅助程序，例如使用人工智能语言对音乐进行动力学建模并以此为基础对表演风格作出分析。这类系统更偏向于对表演过程及要素的学习和分析，距离生成还有一段距离。2000年之后的10~20年是深度学习、自然语言处理、计算机视觉等技术飞速发展是时期，还有了更深度的对人工智能创造力的探索，同时也推动了相关智能技术加速进入艺术领域，形成交叉学科与成果。这个时期出现的辅助手段和技术打破了传统的设计模式和理念，更加的智能化，例如使用人工智能构建自动画手进行创意性作画北京理工大学硕士学位论文4，或者对于零基础游戏机制的设计。近几年可谓是创造性人工智能技术发展的高峰时期，智能创意技术得到进一步发展和应用并一度成为热点话题，成果涉及更广泛的艺术和创意领域，如绘画、电影和写作等，现在的智能生成方法已经能够在部分领域生成较高质量的图像、文本、音乐等作品，并且在很多领域得到了广泛应用。例如，在设计领域，使用智能生成方法快速生成多种样式的设计方案；在表演领域生成音乐旋律、舞蹈动作等元素。从最初的尝试到现在的高度发展，智能创意生成技术已经取得了非常显著的进步，并且为人类带来了很多新的想象空间。随着20世纪70年代以来，数字化融入到各个艺术领域之中，形成了一批以人机交互、数字舞台、表演仿真等技术为代表的交叉应用，表演形式实现跨域的发展和提升。但目前对于创意智能化在表演领域实现真正的广泛应用还相对较远，主要实现方式多是相关表演元素的智能化提升。例如通过软硬件控制器进行智能化舞台表演服饰设计，通过将舞者的肢体语言采集并进行3D成型，利用数据辅助分析舞者做出最佳的表演等。这些技术的深入应用固然能在一定程度上提升表演的呈现度，但真正能够对整个表演产生核心影响的是初始创意阶段的智能化。当前文本、图像、音乐等表演基础要素的智能化生成已经有了不少的研究成果：较早的文本生成技术通常是利用语法规则和词汇库来进行，需要大量的手动工作，随着深度学习技术的发展，基于神经网络的生成方法受到越来越多的关注，应用最广的发展最好的要数encoder-decoder结构。典型技术是Seq2Seq、Transformer结构，无需规则能自动从已有文本中学习如何生成。特别是Transformer框架下一些如BERT、GPT系列、UniLM等大模型的发展和应用都为趣味性和多样性文本的生成带来很多让人惊叹的成果。在创意可视化生成方面，比较具有带有代表性的研究是利用文字生成图片，利用智能算法基于文字生成与之相符的图像，目前主流的方法为图像生成算法，即通过训练神经网络学习文本向量和图像向量的对应关系从而完成映射与合成，一些主流的算法已经可以实现较好的效果比如GAN,StackGAN,AttnGAN,GauGAN等等，已经可以根据文本较好地生成与之相匹配的图像，但是不可避免地在真实性，创意性上有一些缺陷。基于此，我们提出一套真正深入到面向表演领域的创意生成框架，详细研究如何在已有大模型的基础上将创意的自有体系、规则模板智能化地嵌入到创意生成全流程中，让创意生成有条可依。北京理工大学硕士学位论文51.3.主要研究内容与结构框架针对当前研究中存在问题和挑战，本文重点针对如何以现有文化艺术资源构建中国表演创意体系，以及如何对表演创意进行基于情感的智能化生成展开研究。本文主要从以下三个方面进行研究：基于优秀剧本资源的中国表演创意建模方法研究目前为止还没有一个数据集能够涵盖有关中国特色表演的相关文本数据。本文首先构建了一个这样的数据集能够尽可能地填充有关中国表演文本，基于对中国表演创意知识进行结构化的分析与研究，归纳了一套中国表演的元素架构，根据该架构搜集具有中国特色的代表性文化、历史、新闻、传说、经典剧本等文本型数据填充到数据集中保证库中元素架构的完备性。语料的主要来源方式有：网络爬虫、人工收集、网上购入等。然后利用潜在主题建模方法对优秀的创意资源进行挖掘，获得实体相关的主题分布架构。基于情感权重的创意资源知识库搭建本文提出一种加权实体情感知识抽取方法，实现面向文本数据集的结构化情感知识抽取与提炼，为知识库的构建提供可计算、便于管理的基础元素。本文首先实现从大量创意相关文本语料中抽取情感知识，并通过分布计算每个实体的情感要素占比构建实体属性知识候选池，接着通过模式匹配和二部图算法对知识词对进行排序从而筛选出更合理的知识，然后在排名靠后的知识中进行进一步提炼获得知识拓展，最终在所抽取知识的基础上搭建创意资源知识库，创意资源知识库中包含了丰富的主题和情感信息。通过对不同比例的候选知识进行保留和提炼的对比实验，结果表明了知识抽取算法的有效性。多条件约束的创意生成本文提出了一种基于规则和知识的创意生成方法，在创意体系和知识库的基础上，利用这些信息对创意文本生成模型进行控制，生成符合特定主题和情感的创意剧本。本文提出一种基于协同注意力机制的多约束文本生成模型，通过引入co-attention机制，更好地捕捉创意剧本上下文信息，将主题和情感信息作为条件输入，使模型生成符合这些条件的创意剧本，提高生成质量。再通过图像合成手段实现创意的可视化，为计算机更好地辅助艺术家创意创作提供高效智能化工具。全文共分为五个章节，整体框架结构如下图1.1所示：北京理工大学硕士学位论文6图1.1论文总体架构图其中各章节主要研究内容分布如下：第一章为绪论部分，主要介绍了本文的研究背景和意义、国内外相关领域的研究现状、以及本文的主要研究内容和章节安排。第二章为本文的相关理论方法研究，介绍了文本主题分布研究、情感知识提取以及知识库构建、创意生成等相关方法，并就一些主流技术的优缺点进行了横向对比从而确定本文的技术路线。第三章为基于优秀剧本资源的中国表演创意主题建模，详细介绍了如何构建基于网络资源的中国表演创意文本数据集，以及基于该数据集进行潜在语义挖掘的研究成果和实验分析。第四章为基于情感权重的创意资源知识库构建，介绍了本文通过改进的情感实体提取模型从文本数据中提取半结构化知识，并根据实体属性对在文档集中的分布计算各情感属性权重，最终构成创意资源知识库为后续创意生成打下基础。第五章为多条件约束的创意生成，介绍了如何在现有中国表演创意体系和创意资源知识库的基础上分别进行关于主题词的实体和属性扩展，以及在多个约束条件下的创意剧本生成以及创意画面合成。北京理工大学硕士学位论文7第2章相关理论方法研究2.1.引言本章旨在调研当前表演创意生成过程中的主要问题，同时结合近年来在创意话题智能研究、知识表示与抽取以及创意元素智能化生成等领域的应用场景和技术创新，提出一套适用于表演创意生成的方法体系和技术脉络。在当前的表演创意生成过程中，常常面临着缺乏灵感、创意重复性高、难以准确表达想法等问题。为了解决这些问题，本文采用了文本主题研究、知识图谱技术以及文本/图像生成等相关技术，以便更好地挖掘和利用文本数据中的信息和知识，从而拓展创意生成的可能性和范围。在本章中，探讨文本话题研究的技术特征和场景应用模式如何有效解决表演创意生成中的问题。首先分别从基本原理、应用场景以及优缺点等几个层面进行阐述；接着深入探讨知识图谱技术的核心概念和基本构建方法。最后是创意生成相关技术的基本原理和应用场景，以及如何将其应用于表演创意生成中。通过以上的分析和讨论，将形成一套适用于表演创意生成的方法体系和技术脉络，旨在提高表演创意生成的效率和质量，同时推动信息处理和人工智能在表演领域的发展。2.2.文本主题分布研究2.2.1.文本特征表示如文本一类的非结构化数据在被机器处理前需要利用特征提取变成机器可读的数据，文本的特征选取是文本处理、文本语义挖掘的基础操作，它通过量化文本中抽取的特征来表示文本深层信息。将非结构化的原始文本数据转换为计算机可识别和处理的结构化信息，即通过科学抽象对文本进行建模，并用数学模型描述和代替文本，以便计算机能够通过对该模型的计算和操作来实现对文本的识别。文本的特征项需要拥有以下特性:特征可以代表或表示文本内容;特征项具有区分度，并针对每个文本具备独特性;特征项维度不能过高否则加大处理难度;特征的提取要易于实现。在中文文本中可以采用字、词或短语作为表示文本的特征项。对于文本来说，词语比字表达力更强，切分难度又比短语小得多。因此，目前多数文本相关研究中都使用词作为特征项，用来表示文档并作为部分量化指标的计算单位。北京理工大学硕士学位论文8目前面向文本的特征提取主要分为两种：基于统计的提取方法和基于语义的提取方法。基于统计的文本特征提取这类方法以对特征集中每个特征项的评估量化为基础生成面向特征项的权值。文本中对词语所占权重影响较大的多是词频、词性这些因素。也因此针对这些因素的研究是基于统计的特征提取方法的主要思想。基于统计的特征提取方法最为经典也是最有效的是TF-IDF方法。TF-IDF算法是Salton于1988年提出的。其中，TF指词频，用于衡量词的描述或代表能力；IDF指逆文档频率，用于衡量区分能力。TF-IDF算法基于一个主要思想：在一个文本中多次出现的词语，在另一个相似的文本中也会多次出现。因此TF可以反映同类文本之间的共性。此外，TF-IDF认为，在不同类别中出现频率较低的词语具有更强的区分能力，因此引入了IDF概念，并将其与TF相乘作为特征空间坐标系取值测度。然而，TF-IDF多用于不考虑词的位置信息对语义有影响的场景。文档频数是一种简单的特征选择算法，用于衡量一个单词在整个数据集中出现的次数。在训练文本集中，对每个特征计算其文档频率，并根据预设阈值去除文档频率过低或过高的特征。由于文档频率可以通过线性近似复杂度来计算巨大的文档集，因此该方法适用于任何语料库，并且具有较低的计算消耗，常被用作特征降维技术。还有一种适合复杂场景的N-Gram算法，它的基本思想是滑动窗口，即在文本序列上进行一定长度的滑动窗口操作，从而形成等长的字节片段序列。每个字节片段就是gram，对所有的gram的进行出现次数统计，再经阈值过滤形成映射表。该映射表就是该文本的特征空间，每种gram对应一个特征向量。N-Gram因其不用分词的特点具有广泛应用性。但也因定长滑动窗口的特性，会导致较长词语在被切割时产生语义偏差，因此在一些专业领域的应用需要进行衡量。基于语义的文本特征提取基于统计的提取方法大多考虑的是以词为单位的特性，不考虑整篇文档更深层次的信息，但是在有些研究中还需要考虑语句的含义和内在关联，这时候就需要进行基于语义的文本特征提取，最经典的是基于HowNet的提取方法。通常在对文本的处理中，尤其是中文的字、词和短语作为文本的基础构成要素必定是主要的处理对象。北京理工大学硕士学位论文9但是这些特征更多地反映了文档的词汇信息而非语义信息，因此无法准确表达文档深层内容。再加上向量正交通常是向量空间需要遵循的基本规则，但在文本中词汇之间存在着一定程度上的相关性，这与规则是不相符的。基于概念的特征提取可以解决这个问题,主要是利用HowNet获取词汇的语义信息,并通过聚类将概念相似的词合并。经聚类后的词作为文档特征项,大大降低了特征空间内不同项之间的相关度，能够比基础的统计方法更准确地代表文本。2.2.2.主题建模进入大数据时代，数据和信息量猛增，这其中绝大多数数据是非结构的，要想获得想要的信息越来越困难。庆幸地是，出现了一些有效的方法来帮助我们从这些复杂数据中归纳出我们想要的信息。文本挖掘领域中的一种关键技术是主题建模。这项技术用于自动分析文本数据中的主题和关系，能够从一个文本集中识别主题分布，发现隐藏的模式，帮助后续更好地决策。主题建模和那些基于规则的文本挖掘方法例如基于正则表达式或者基于字典的关键词搜索技术不同，它是一种无监督方法，用来从大规模的文本集中发现主题或者说词群。主题模型在文本聚类、大规模文本数据组织、基于非结构文本的信息检索以及特征选择方面非常有用。例如，纽约时报使用主题模型来强化他们的用户文章推荐引擎。一些公司在招聘行业使用主题模型来抽取工作描述中的潜在特征并将他们和合适的应聘人匹配。主题建模还被用来组织大量的邮件、消费者评论以及用户社交媒体档案等等。以下是几种常用的主题建模方法：狄利克雷潜在主题分布是一种常用的无监督机器学习主题建模算法，可以将文本数据分解为多个主题，每个主题包含一组单词。LDA基于概率图模型和贝叶斯推断，通过对数据建模发现主题。可以用来识别大规模文档数据集或语料库中潜藏的主题信息，特别是对于普通文本这样的非结构化数据，LDA主题模型是用于检索文本信息最简单和最广泛使用的数学模型，通过对数据进行贝叶斯推断来确定每个文档包含哪些主题以及每个主题在文档中的权重。在得到这些主题和权重后，可以对它们进行解释和分析，以帮助后续更好地理解文档中的主题和模式。LDA的优点在于可以处理大规模数据集；可应用于多语种和多主题域的文本数据；并且在目前的众多实践中表现出了很好的效果。但也有一些缺点例如对于一些高度特化的应用，效果可能不如其他方法；训练时间较长，需要高性能计算设备支持。北京理工大学硕士学位论文10概率隐语义分析也是一种常用的主题建模算法，与LDA类似，可以将文本数据分解为多个主题。不同之处在于，PLSA使用非贝叶斯方法，通过最大化似然函数来寻找主题。其优势在于训练速度快；可以处理大规模数据集。缺点在于需要手动确定主题的数量；容易受到噪声数据的影响。隐含狄利克雷分布是一种非参数主题建模算法，可以自适应地确定主题的数量和结构，不需要手动指定主题数量。HDP可以看作是LDA的扩展版本，能够更好地处理大规模数据集。缺点除了训练需要资源的耗费，还有在某些情况下，HDP可能会生成太多或太少的主题。矩阵分解是一种广泛应用于推荐系统中的主题建模算法，可以将用户评分矩阵分解为用户和物品的主题分布矩阵，进而预测用户对未评分物品的评分。其优势在于训练速度快；可以应用于推荐系统和评分预测；缺点是只适用于处理稠密矩阵且不适用于文本数据。双层主题模型是一种深度主题建模算法，可以对主题进行多层抽象，从而更好地描述文本数据中的复杂结构。优点是可以对主题进行多层抽象，从而更好地描述文本数据中的复杂结构；缺陷则是很可能会过度拟合，需要进行模型优化。2.3.知识库知识库是面向知识搭建的系统数据库。许多应用程序都利用知识，将知识进行系统地组织并形成独立的库实体不仅可以高效统一地进行管理，还能提升知识库的复用性。在通用知识库的构建方面，目前学术上已有一套相对比较成熟的技术体系，市场上也有多款商用知识库投放，如谷歌的“GoogleKnowledgeGraph”、百度的“知心”等。从目前的研究来看，知识库作为知识系统的一种表示形式，与知识图谱的相关特性与技术拥有部分共通的特性，因此，本节借助构建知识图谱的基础技术在一定程度上探索知识库的构建方法。知识图谱的概念最早是在二十世纪中由普赖斯等人提出，他们希望利用引文间的互联关系来研究当时科学发展脉络由此构建了图谱的网状结构，到21世纪，随着网上信息爆炸式增长，传统的文档式信息组织方式已经不能满足人们的检索需求，由此不少学者和机构开始在知识图谱上深入研究以求获得更清晰、动态的知识组织管理方式。北京理工大学硕士学位论文112.3.1.知识库构建技术构建知识库的主要目的是获取大量的、让计算机可读的知识。过程中需要涉及到知识表示与推理、自然语言处理、数据挖掘等方面的多种技术。知识图谱的构建根据知识形成方式主要分为自顶向下和自底向上两种。自顶向下是从数据源中学习到术语、关系等基础概念及关联关系,然后再将学习到的实体纳入到已有的概念体系中。自底向上则与之相反,从实体入手一步步地归纳、抽象,最终形成分层的概念体系。在构造过程中,也有研究通过混合两种方式提高知识抽取的准确率。当然无论采用哪种方式，知识抽取都是最关键的技术之一，其输入通常是非结构化文本或者图像视频等数据，通过自动化或半自动化的技术抽取可用的实体、关系及属性等知识要素，并以此为基础形成高质量的事实表达。由此知识提取技术主要面向的也就是这三个方向：实体抽取、属性抽取和关系抽取：1）实体指的是具有可区分性且独立存在的某种事物。例如城市、地点、植物等等。实体抽取也称为命名实体识别，实体抽取的准确性直接影响之后构建的知识库质量。基于目前的研究可以将实体抽取的方法分为4个阶段：早期的抽取主要基于规则或词典驱动，这种方式更依赖于模板导致效率低下；然后出现了基于传统机器学习的方法，利用机器对大量语料的学习进行抽取；接着是基于深度学习的抽取方式，具体的研究成果更多，效果也更好；最后是基于开放域的抽取方法，主要面向大规模的中英文网络语料。2）属性抽取也是知识库构建中的重要一环，其难点在于需要首先构建属性结构，因为属性名和属性值是对应的关系，不能混乱，因此很多研究都是利用规则模板的匹配进行抽取。这种方式在很多对知识准确度要求比较高的场景中存在一定的弊端，不仅噪声信息多，而且很难发现规则外的新属性。也因此有研究利用深度学习方法将属性抽取抽象为序列标注问题，通过片段式的机器阅读理解模型完成更高质量的属性抽取。3）关系可以看作是一个函数，将实体和属性连接起来形成图，可谓是构建语义关联的关键一环，关系抽取的难度在于文本语句的结构复杂性，实体和属性出现的位置、中间连接语句和前后关系等影响因素多种多样，因此难以利用模式匹配这样较为死板的方法进行关系抽取。关系抽取方法的发展阶段也和前两个要素相似，但在更深层次的语义信息上一些研究者提出了基于马尔可夫逻辑网、基于本体推理的深层隐含关系抽取方法。北京理工大学硕士学位论文122.3.2.知识库存储如何存储这样一个庞大的网状图谱是我们接下来要面对的问题，这时候传统的关系型数据库在针对实体属性关联性查找及延伸方面暴露出较大的弊端，而原本就支持关系型数据的数据库是最好的选择，例如：图数据库。这是一种面向图存储结构优化的数据库管理系统，主要用于对图类数据的增、删、改、查，并且可以更直观地存储和展示节点之间的互联关系。结构特性意味着它不仅可以带来运行性能的提升，还能大大减少维护成本。目前对图数据库的研发升级越来越多，其中应用较广的有Neo4j、TigerGraph、JanusGraph、MangoDB、OrientDB这几款图数据库：Neo4jNeo4j是一个高性能的NoSQL图数据库，实现专业级别的图数据模式存储，具有“免索引邻接”的特点。Neo4j是基于java语言环境开发的，在程序部署和运行上都具有便捷易用的优势，而且和普通的图存储数据库相比，Neo4j提供更加完整的数据库特性支持，例如对ACID、高可靠集群部署等的支持。除此之外，Neo4j的使用依赖于名为Cypher的图查询语言，该语言具备人性化的可读格式，拥有较高的查询效率。且Neo4j提供的RESTAPI可以支持任何编程语言的接入和访问。TigerGraphTigerGraph是一个企业级可扩展图数据库。TigerGraph的成熟技术连接了数据孤岛，可进行更大规模、更深入的运营分析。可以分析数据中的深层关系，从而提供重要业务洞察功能。TigerGraph拥有速度快，数据压缩效率高等优势，因此与其他替代产品相比，它使用的计算和内存更少，降低综合成本。JanusGraphJanusGraph是一个开源的分布式图数据库。具有良好的扩展性，支持用户高并发实时访问和查询。JanusGraph最大的优势是：可以扩展图数据的处理，能支持实时图遍历和分析查询。因为JanusGraph是分布式的，可以自由的扩展集群节点的，因此，它可以利用很大的集群，也就可以存储很大的包含数千亿个节点和边的图。MangoDBMongoDB是一个基于分布式文件存储的数据库，由C++语言编写。旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB是一个介于关系型和非关系型之间的数据库，在非关系数据库中功能最丰富。MongoDB以文档的形式组织存储数据，数据结构由键值对表示。北京理工大学硕士学位论文132.4.创意生成目前将智能计算应用到表演创意方向，实现创意的自动激发与生成还处于发展初期，但也有一些技术的磅礴发展让我们看到智能化是表演创意的必经之路，例如文本生成和图像生成。2.4.1.文本生成目前还没有专门面向表演创意的文本生成模型，但是基于文字续写、文档仿写、机器翻译、智能问答的模型与技术已经有了很好的发展。文本生成技术是自然语言处理领域中的一个重要分支，旨在利用机器学习和深度学习等技术生成高质量的文本内容，如新闻、评论、诗歌等。目前，主流的模型包括基于规则的模型、基于统计的模型、基于神经网络的模型等。其中，基于神经网络的模型，如循环神经网络、长短时记忆网络、Transformer等，都在文本生成方面表现出了卓越的性能。目前人工智能技术对表演创意文本的生成研究，从生成流程的各任务节点关键性进行归纳总结，大致可从三个方面展开：从对输入的理解方面来说，如何充分理解输入是创意生成的首要任务。目前面向文本生成任务的输入形态包含非结构化输入，结构化输入和多模态输入三种。在文本生成中，大多数研究集中在非结构化文本输入上，这需要准确地理解输入并获得有意义的信息表示。非结构化输入可将段落视为句子的集合，将句子视为词的序列。因此为了同时获取段落中低层的词义和高层的主题语义，许多研究提出了基于层次结构或基于图表的段落表示学习方法。如Gu等人提出的DialogBERT使用面向句子的编码方法，用一个编码器表示对句子的聚合；Li等人提出了建模对话动态信息流的方法DialoFlow，引入动态流机制通过处理每个话语带来的语义影响来模拟整个对话历史中的动态信息流。Liu等人提供了一个基于BERT的篇章级编码器，能够编码一篇文章并获取每一句话的表征。Alexis等人将生成预训练模型的方法迁移到多种语言从而展示了跨语言模型的有效性。从训练模型设计的角度，在将输入数据编码为低维表示之后，下一步是开发一个有效的预训练模型作为文本生成函数。经典的结构主要为四类：首先是MLM掩码语言模型使用完全注意力机制的Transformer编码器。在充分注意的前提下，模型通常采用掩码语言建模任务进行预训练，即利用双向信息预测掩码标记。其中最具代表性的是BERT模型，广泛应用于自然语言理解领域。北京理工大学硕士学位论文14有研究把BERT、RoBERTa、GPT2三个模型混合起来构建生成模型表演优异。其次是单向语言模型，近期大火的GPT系列是其中的经典，还有CTRL等条件生成模型的成果也很可观。但由于这类模型本身从左到右的限制，在文本摘要、翻译这种需要双向信息的任务上表现欠佳。第三种是前缀语言模型，这类模型兼具双向编码和单向解码，比如UniLM系列、GLM。不过研究显示，比起经典的encoder-decoder结构还是稍稍逊色。最后一种是编解码语言模型，该模型遵循用于文本生成的标准Transformer体系结构，由编码器和解码器层的堆栈组成。在预训练过程中，MASS和ProphetNet将带有一个掩码段的序列作为编码器的输入，然后解码器以自回归的方式生成掩码令牌。从生成模型优化的方向来讲，因为生成主题、应用领域等因素，为了获得更良好的性能，开发基于文本预训练模型的有效优化算法是至关重要的。我们考虑三种主要的优化方法，即微调、提示调优和属性调优。Finetune主要面向的问题是监督数据太少，特别是针对创意生成领域，目前比较多的预训练模型是基于大量通用语言学习的，事实型数据多，发散型性数据少，此时使用具有目的性的数据进行Finetune就显得尤为重要。Prompt是今年最热的NLP方向，最初是GPT2发起的，在输入中加入对任务的描述，比如，这样一个生成模型就可以同时做多个任务。不过最初这种形式很依赖人工设计的prompt，所以之后的研究者也提出了AutoPrompt等自动发现模版的方法，再演变到后来就是参数化的ContinuousPrompt。除此之外，还有一种方式用来限制生成结果的部分性质，这在面向特殊任务进行生成时也格外具有优势，属性调优的目的就是限制生成结果的一些性质，比如有的方法会通过TF-IDF选择掩码哪些词，强迫模型生成更相关的结果。创意文本生成的研究还面临一些挑战。例如，如何保证生成的文本的创意性和质量，以及如何在生成文本中保持主题的一致性和情感的连贯性等问题如何提高生成文本的多样性和创新性，如何解决数据稀缺和过拟合等问题。此外也面临着一些伦理和法律方面的挑战，例如如何避免生成有害的内容，保护用户隐私等。2.4.2.图像生成在创意图像生成方面，已经能够自动生成高质量的图像、图标、艺术作品等，这些方法主要基于深度学习和计算机视觉技术。其中生成对抗网络、变分自编码器和扩散模型是近年来创意图像生成方面应用比较广的几种：北京理工大学硕士学位论文151、生成对抗网络由生成器和判别器两个网络组成，其中生成器用于生成仿造的图像，而判别器则用于判别图像的真伪。生成器和判别器分别通过对抗训练的方式进行优化，直到生成器生成高质量的仿造图像，使得判别器难以区分与真实图像的差别。GAN模型具有较高的创意性，能够生成具有高度创新性的图像，并通过对抗训练的方式不断优化生成器和判别器，进一步提高生成图像的质量。2、变分自编码器是一种基于自编码器的生成模型，可以用于学习数据的分布。它将输入数据通过编码器网络转换为潜在向量，然后通过解码器网络将潜在向量转换为生成图像。与传统的自编码器不同，变分自编码器通过在编码部分引入潜在变量控制生成图像的属性，比如色彩、姿态、表情等。因此，VAE可以用于生成多样化的图像，从而具有较高的创意性。VAE模型的核心是损失函数，通常采用最大似然估计的方式进行优化。由于最大似然估计仅考虑了数据的重构误差，而没有考虑潜在变量的分布，因此VAE模型在生成多样化的图像方面仍然存在一些局限性。近年来，一些研究人员提出了基于VAE的改进模型，比如 -VAE和AnnealedVAE等，这些模型通过引入不同的损失函数，进一步提高了VAE的生成效果。3、扩散模型是近年来在创意图像生成方面备受关注的一种神经网络模型。与传统的生成模型不同，扩散模型使用迭代式的扩散过程来生成图像，可以生成高质量、多样性和具有创意性的图像。扩散模型基于马尔科夫链理论，将图像的生成过程视为一个时间序列问题，其中每个时间步骤都会产生一个中间状态。通过对每个时间步骤的中间状态进行反向传播，不断更新生成图像的概率分布，从而生成高质量、多样性和具有创意性的图像。扩散模型的优势在于生成质量高，同时也能够控制图像的属性。此外，扩散模型还具有较高的可解释性，可以通过中间状态来理解生成图像的过程。需要注意的是，扩散模型相较于其他生成模型，其计算成本较高，在训练过程中需要消耗大量的计算资源。2.5.本章小结本章主要介绍了研究框架中主题分布、知识库构建、创意生成相关技术的发展现状及横向对比，然后基于这些相关理论，针对表演创意方法的现实需求，提出了以基于LDA的中国表演创意体系建模和基于情感权重的创意资源知识库方法为底座，以多条件约束的创意生成为主线的表演创意生成框架。该框架能够在现有创意体系规则指导下完成更富有情感要素的表演创意生成。北京理工大学硕士学位论文16第3章基于LDA的中国表演创意建模方法3.1.引言创意的最初形式是创意元素按照一定规则的堆叠，这个过程包含两个关键：一是创意元素的选择，二是堆叠的规则依据。目前的创意产生过程大多还是根据导演等艺术创作者的经验和积累进行，虽然这种方式有可能迸发高质量的创意，但也存在一些缺陷：一是过于依赖个人经验和直觉，产生过程需要艺术家具备丰富的知识、经验和灵感，可能会限制创意的多样性和创新性；二是时间和精力成本高，依赖于人力的方式使创意的产生过程变得缓慢。三是受限于个人主观意识和审美观，不同的艺术家可能会产生效果不同的创意，受个人作品质量和风格限制，从而导致创意的片面性和局限性。而智能化手段虽然克服了一些人力资源消耗和主观素材受限的问题，大多还是通过机器学习等技术从大量数据中学习，然后采用模拟、仿照的方式进行创意生成。生成过程缺少了对创意元素和创意规则的深入挖掘。因此，在智能化创意生成领域，我们需要深入研究人类创意产生的规则和心理机制，探索创意生成的本质，从而更好地引导机器学习生成具有独特性和创新性的创意。本章希望利用LDA模型对中国表演相关的创意文本进行主题建模和主题挖掘从而构建基于创意元素潜在语义关系的中国表演创意体系，以便后续依据该体系规则进行创意的生成。本章研究结构如下图3.1所示：图3.1第3章研究架构图北京理工大学硕士学位论文173.2.中国表演创意文本数据集构建在对中国表演剧本进行潜在主题研究之前，建立一个可靠的数据集非常必要。因为数据集是机器学习的基础，且构建数据集可以帮助模型更好地对比信息，发现其中的规律和特征，从而为后续的数据挖掘提供思路。数据集是指一个大量数据的集合，可以是文本、图像、音频等，具体元素类型由分析对象决定，一个好的数据集应该具有代表性、多样性和完整性，能够反映出真实的数据分布和特征。而在进行面向表演创意的潜在主题研究时，构建一个涵盖中国表演剧本的数据集，可以为分析这些剧本的主题和元素提供有力支撑。但是目前为止，可以说还没有一个数据集能够涵盖有关中国特色表演的相关文本数据，因此本文旨在构建一个这样的数据集能够尽可能地填充有关中国表演的文本语料，以供后续使用。为了该数据集的构建，本文率先致力于对中国表演创意知识进行结构化的分析与调研，从而构建了一套专门面向中国表演的元素架构，并根据该架构搜集具有中国特色的代表性文本。语料的主要来源方式有：网络爬虫、人工收集、网上购入等。这其中的首要对象是：情节模板类：包括优秀剧本、演出脚本、表演记录以及情节故事等；这类文本通常具有丰富的情节和较强的故事性，每个文档内部即包含一套完整的故事逻辑和发展脉络，且因为这类文档的情节连贯性，内部实体及属性在情感上也更具统一性。同时在收集过程中发现两方面问题：一是现有的剧本脚本类资源相对较少，另一方面，表演是文化的载体，这些剧本和故事类并不能全面覆盖我国丰富的表演文化内涵，再加上表演创意需要新的元素去填充而不是仅仅仿照已有的优秀剧本。因此本文根据剧本创意可能需要的要素以及富含中国文化内涵两大原则进行搜集范围拓展，拓展的语料主要为3类：场景描述类：以中国境内代表性地点、景点的导游词以及优秀的风景散文、记叙文为主；文化符号类：以中国传统神话传说、经典历史故事、各地典型民俗风俗为主；架构要点补充类：以表演评价性语料、表演相关新闻报道以及经典大型活动流程解说词为主。这四类文本型数据构成中国表演创意文本数据集，以此来保证库中元素架构的完备性。本文以网络资源为主要数据来源，以围绕中国表演创意元素的直接或间接来源北京理工大学硕士学位论文18文档为研究对象，共收集8650篇文本预料，语料分布情况如下表3.1所示：表3.1语料分布情况类别语料文档数情节模板类经典故事、剧本脚本1591场景描述类风景散文33各省代表性地点导游词401文化符号类经典民俗67神话传说513架构要点补充类表演评价1021表演相关新闻报道5478奥运会解说词6该数据集不仅可以从中抽取相关实体和关系来填充创意知识库，还能够从中获取文本生成模型的训练语料，使得模型能够更专注于对于中国表演创意的学习与生成。3.3.创意文本数据预处理数据预处理可以将无序、杂乱的数据处理成机器直接可读的信息，在本节中主要由数据过滤和特征映射两个操作构成。3.3.1.创意文本数据过滤大到整个信息海洋，小到一份简单的财务报表信息，不同的研究领域，不同的研究方法都需要过滤掉无效信息数据后，才能利用剩下的有效信息进行更符合主题的研究。在当今信息泛滥的时代，这种现象更是格外突出，因为大数据时代，产生的数据多以非结构化数据为主，需要经过数据预处理的过程转化成结构化或半结构化数据才能进行后续计算，前文收集的创意文本数据集中的语料就是典型的非结构化数据。数据过滤是指采用一定的方式将与主题信息无关或影响后续处理的数据筛选出去，使得数据更加干净，方便后续的分析、建模等步骤。如果缺少数据过滤，后续的分析、建模等步骤将无从下手，即使强行进行，得到的结果也会受噪声信息影响从而不准确。常见的过滤操作包括删除特殊符号、去除字符间多余的空格、删除连续出现的标点等。北京理工大学硕士学位论文19由于目前构建的中国表演创意文本数据集中的数据主要是通过网上资源获取。杂乱的信息噪声混入其中，同时又因其非结构化的特性，初步完成的分词存在很多对于创意体系建模来说价值极低的词，其中包括：特殊字符、特殊格式、无意义字符、低信息量字符等。直接进行文本特征构造，会使得同尺寸特征中关键信息的比重下降。特别是在构建主题模型的的过程中，发现“的”“地”“得”这样的词语无助于表达一个主题。由此本文引入停用词表进行有效信息的过滤，停用词表共包含1893个停用词，部分停用词如下图3.2所示：图3.2部分停用词示例进行信息过滤后的效果如下图3.3所示：图3.3过滤结果3.3.2.文本特征映射文本特征表示的过程是将在计算机看来是字符串的文本转化为在数学上处理起北京理工大学硕士学位论文20来更为方便的向量。目前进行文本特征构造的方式主要是基于统计的特征提取方法，基础模型是词集模型，意为单词构成的集合，词袋模型是在词集的基础上增加了频率的维度，暂时不考虑词与词之间的语义级关系，只通过出现频率衡量词的权重。在进行特征提取之前首先要进行分词，分词作为文本处理的基础操作，可以帮助机器识别语句含义。目前为止英文在很多语言任务中表现要优于中文，一部分原因就是中文分词比英文要复杂、困难得多，因为在英文的行文中，单词之间拥有空格这样的天然分界符，而中文由于继承自古代汉语的传统，没有词层级上的明显分隔，所以更有难度。特别是中文里许多字词的结合是灵活多变的，比如“打碎”既可以看做一个词，也可以看做一个短语，这就导致对词的界定略显模糊。在提取时，未分词的中文文本因为没有分隔符，所以提取特征时会自动的将一段不含空格的句子作为一个特征，这样的话就难以达到有效提取的效果。例如在处理例1这样一个句子集合时：例1：'雄伟壮观的万里长城是人类建筑史上罕见的古代军事防御工程。''北京的故宫是世界上最壮观最雄伟的宫殿之一。''八达岭长城位于北京市延庆区。''北京故宫是中国明清两代的皇家宫殿。'初始的统计结果是将整个句子视为一个特征维度，这种情况下特征矩阵较小，单词统计的稀疏矩阵表示如公式：A=0,311,212,013,11将A转换为密集矩阵如公式所示：B=0000011010010000密集矩阵对应特征维度为。但是矩阵B的有效信息太少，难以从中计算潜在语义，由此尝试分词后的特征提取效果，重新计算的密集矩阵B'为：北京理工大学硕士学位论文21B'=000100101101010001111101000010110001000001000011010000100000100010000010010001110000对应特征维度则为。这样每个句子的特征维度更丰富，通过词间的频率统计也更容易发现不同句子之间的关联。3.4.主题建模LDA主题模型可以对文本数据进行语义层面的信息挖掘。它的建模方式是将整个数据集看作满足一定概率的主题分布，再往下细分把每个主题看作是词语的分布，有效降低了非结构化数据的维度。LDA主题模型由文档、主题、词语三层贝叶斯结构构成，模型结构如下图3.4所示。该模型广泛应用于文本分类、信息检索、主题挖掘等文本处理的主要领域。图3.4LDA主题模型结构从上图可以看出，LDA对文档进行主题研究是通过获取词和主题间的概率分布进行的。文档集合D中的每个文档d可以看做是一个单词的序列，wi表示第i个单词，d共有n个单词，则D中的所有单词组成一个大集合V，V中共包含m个词，而主题集合构成T，假设最终输出k个主题。LDA模型是一个基于贝叶斯模型的无监督机器学习模型，满足的基本原理为：先验分布+数据=后验分布LDA假设文档主题的先验分布符合Dirichlet分布，即对于文档d来说，其主题分布 d满足：北京理工大学硕士学位论文22 d=Dirichlet其中 为该分布的超参数，维度为k： =主题中词的先验分布同样，也就是说对于主题t，它的词分布 t为： t=Dirichlet则 同样是该分布的超参数，维度为m： =基于狄利克雷分布和多项分布的共轭性，那么对于任意一篇文档中的某个词，就可以得到它的主题编号的分布为：zij=multi同理，对于某主题编号，特定词的概率分布为：wij=multi其中i为文档编号，j为文档i中第j个词，在此式中 d zd就组成了Dirichlet-multi共轭，因此可以使用贝叶斯推断法计算出文档主题的后验分布。如果在第j个文档中，第t个主题有njt个词，则对应的分布计数为:nj =nj1,nj2,…,njk则利用共轭特性，得到 d的后验分布：Dirichlet同样的道理，对于主题与词的分布，有k个主题与词的Dirichlet分布，而对应的数据有k个主题编号的多项分布，这样 t wt也组成了Dirichlet-multi共轭，如果在第t个主题中，第i个词的个数为nti，则对应的多项分布的计数可以表示为:nt =nt1,nt2,…,ntm则可以得到 t的后验分布为：Dirichlet3.5.实验分析本文实验采用LDA模型对极具中国特色的表演创意文本数据进行主题挖掘与体系建模，流程如图3.5所示：北京理工大学硕士学位论文23图3.5实验流程图3.5.1.最优主题数确定在LDA主题建模方法中，首先需要确定主题数量。主题数量直接影响模型质量以及之后的体系构建。本文采用困惑度和一致性两个度量指标，借助不同主题数量下的模型合理性找出最优主题数。其中困惑度表示模型识别某文章属于某主题的不确定性的大小。因为困惑度越低，不确定性就越小，则最后的聚类结果就越好。与困惑度相反，更高的一致性表示更好的可解释性，和语义上更连贯。因为待处理的数据量较大，每次进行一个k值对应的困惑度和一致性计算都相当于要进行一次模型生成，本文首先选取主题数k=20进行LDA试分析并使用pyLDAvis进行可视化来确定大致的主题分布范围，可视化结果如图3.6所示：图3.6主题数k=20的LDA建模结果北京理工大学硕士学位论文24上图3.6中每个圆圈代表一个主题，圆圈的大小表示该主题在文档集中出现的频率，主题之间的位置远近则代表主题之间的相近性，上图可以看出当k=20时不同主题重叠度比较高，而根据主题分布的大致区域和距离由此确定主题分布范围是10以下的。由此进行不同主题数，模型生成后的困惑度以及一致性的对比评估：图3.7主题困惑度计算结果从上图3.7可以很明显地看出在主题数k=5时困惑度最小，接下来使用一致性进行进一步验证：图3.8主题一致性计算结果在上图3.8中同样当k=5时一致性为范围内最高值，因此综合两个指标，可以确定选取主题数k=5来进行正式的LDA建模分析能够达到基于此数据集的最好效果。北京理工大学硕士学位论文253.5.2.主题模型分析经过主题数为5的建模过程，本节我们利用对主题词的统计分析进行主题体系构建，通过对每个主题代表性词语的内容进行基于经验的内容标注归纳出各个主题，结果如表3.2所示:表3.2表演创意研究主题主题编号主题词主题内容标注t1师父；母亲；问道；宝贝；老爷；模样；手段；天神；将军；可怜；怪物情节t2太监；颐和园；烩面；历史；戏曲；正厅；嘱托；秋日；权势；祭祀历史t3文化；活动；传统；技艺；制作；历史；艺术；发展；传承；文化遗产文化t4城隍；推拿；绒绣；宣卷；山土；南音；字体；琵琶；白杨；郁金香艺术t5舞剧；群舞；编导；情感；结构；生命；塑造；叙事；时代；古典时代主题和词分布结果将用于后续创意生成时的首要步骤：面向主题词的实体选择，完成根据主题词搜索实体意象搭建创意基础骨架的目的。此外，对主题分布进行可视化结果如图3.9：北京理工大学硕士学位论文26图3.9主题数k=5的LDA建模结果可以看出目前的表演主要内容集中在“情节、文化、艺术、历史、时代”这几个方向，而又根据可视化结果“文化”、“艺术”、“时代”三个主题的意象相比其他两个主题距离更近。因此在后续的创意生成时部分意象可以考虑相互拓展。3.6.本章小结本章主要介绍了对创意元素的数据收集和创意建模分析的过程。首先对在中国特色表演中出现的创意元素及类型进行调研，确定需要进行创意体系构建的语料种类，通过爬虫、搜集、购入等多元方式将涵盖这些类型的语料汇总构成文本数据集。第二步是对这些初步收集的语料进行分词和数据过滤，避免噪声语料对后续挖掘和研究过程的干扰。第三步是在语料基础上利用LDA主题模型进行主题分析，研究目前创意表演中的主要元素的分布状态和规律，从而完成以中国表演创意为基础的创意建模。北京理工大学硕士学位论文27第4章基于情感权重的创意资源知识库4.1.引言情感是人类交流和交互的核心，情感要素能够帮助表演创意更加生动。在第三章中我们已经讨论了表演创意体系的主题分布情况，主题下分布着的大量实体是创意生成过程中重要的意象来源，而进行创作的过程中需要保证意象情感的丰富性和一致性，成功的创意往往在情感上更具感染力，从而引发观看者的内心共鸣。最典型的成功案例要数本次北京冬奥会闭幕式中的，演员拿着发光的“柳条”，在的音乐中缓缓地向中间走，有无数道绿色的光像纪念碑一样升起来。“柳枝”的意象蕴含“思念”的情感，“绿色”又蕴含着“希望”的情感。这是独属于中国人的浪漫，丰沛的感情，含蓄地表达，久久的回味。再加上近年来的大多数表演或者活动在最初确立主题时往往是一个或几个情感词，创意团队需要根据这些情感词进行意象的联想和扩充，目前的知识库多以包含事实性知识为主，导致辅助创意创作时情感方面有所欠缺，因此为了保证创意要素的丰富性，构建一个富含创意要素的情感知识库是十分必要的。因此本章针对这一问题，提出了一种基于文本语料的加权情感知识提取方法，旨在从非结构化的中文语料中提取实体情感知识，并通过知识在语料中的分布情况进行情感加权，从而构建基于情感的加权知识库。本章研究结构如下图4.1所示：图4.1第四章研究架构图北京理工大学硕士学位论文284.2.知识抽取知识抽取，即从不同来源、不同结构的数据中进行知识提取，形成知识存入到知识库或图谱中。知识抽取的难点在于对知识源中的数据进行处理，因为知识并不是以某种现成的形式存在于知识源中的，只有对知识源中的数据经过分析、识别、理解、关联等一系列处理之后，才能发现其中有用的知识。而这种数据处理往往又因为知识源中数据形式的不同而呈现不同的难度。本节旨在提出一种从非结构化文本中抽取知识构建知识候选池的方法。4.2.1.构建知识候选池从逻辑学的角度，基本的形式概念可以分为实体和属性。实体指能够独立存在的具体事物或个体，在语句中主要是名词形式，属性则是以事物的性质和关系为反映对象的概念，属性的词性选择和其他研究者的做法一致，以形容词为主要形式。因此抽取的过程首先要进行分词和词性标注，主要进行标注的词性和示例如下表4.1所示：表4.1需要进行标注的词性及示例标志nnrnsntnzsvna词性名词人名地名机构团体其他专有名词处所词名动词形容词示例兰花王明北京工会西游记后院改革美丽构建知识候选池的主要工作就是根据词性进行实体和属性的配对，实体和属性的位置虽然会具备天然的趋近性，但距离和前后关系并非是固定的，比如下图4.2中的两个对比示例：图4.2实体属性两种位置关系因此本章设定以句子为抽取单位，将每句话中的实体和属性抽取之后进行排列组合，例如从“这座山峰很壮丽而且主峰很高”中可以抽出四条初始知识，见表4.2：北京理工大学硕士学位论文29表4.2排列组合得到的初始知识列表序号候选知识对①山峰—壮丽②主峰—高③山峰—高④主峰—壮丽当然在这个例子中的情感属性在排列组合后也能够较好地适配，但在实际处理过程中也会遇到各种表达方式，除了图4.2例子中位置不同但实际意思相同的两条知识，还有例如：“高高的山峰”，“山峰的具体高度是多少”这两个句子中出现“山峰—高”“山峰—具体”这对看起来位置相同但意思和适配度大为不同的组合。这就需要进行进一步的筛选，为了在筛选时能够更好地进行量化评判，我们观察到实体和情感属性的关系主要是两种模式：“实体…联结词…属性”和“属性…联结词…实体”，在实际抽取时，我们采用模式匹配的方法对两个模式加以“”的标记区分属性对于实体来说是前修饰还是后修饰，那么“实体i—联结词k—属性j”就代表两条不同的候选知识。4.2.2.实体情感权重计算仅仅是找到实体和属性的配对关系还远远不够，为了保证知识的准确性和丰富性还需要进行筛选和权重计算。知识筛选是为了解决在上节中为了获得足够多的候选知识填充侯选池，其中出现一些错误的搭配的问题；权重计算则是因为一个实体往往对应许多的情感属性，仅仅是实体与属性的联结对于文本的深度语义挖掘还不够充分，我们希望通过研究每条知识在文本语料中的分布情况获得每个实体的情感权重。候选知识出现的频率直接影响这条知识的可信度以及此项属性的情感比重，但是一个文档具备天然的上下文一致性，一条候选知识很可能在同一文档中反复出现从而拉高频率，如下图4.3所示两条知识都出现了10次，但是显然出现在更多文档中的知识B比A更加可信：北京理工大学硕士学位论文30图4.3知识分布示意图：A知识B知识因此这里我们提出基于多层次分布计算的知识加权方法，借助TF-IDF方法的逆向思维进行词频统计和权重计算。原始的TF-IDF适用于分类，主要思想通过统计某个词在文档篇章内和篇章外不同层次的出现频率界定它的类别区分能力。我们的逆向思路则是文档集D种共有n个文档:D=其中对于每个文档di包含m条知识：di=假设知识pj来说在文档di中出现的频次 ij，在由此可以得到基于知识分布的文档集特征：FD= 11 n1 1M nM其中M表示D中提取的知识总数量，以每条知识为单位计算占比度和稳定性：ESD=1nk=1n k1 1/ 1nk=1n kM 1/总出现次数越多即占比度越高比重越大，包含的文档数越多即稳定性越高，比重也会更大。由此得出知识pj的相对比重为：Spj=k=1n kj q=1n2 假设实体Er拥有u次属性匹配，进行归一化处理，每条知识针对Er的情感权值即为：wrj=Sprjl=1uSprl 4.3.知识提炼在第二节中我们通过抽取大量的情感知识构建了知识候选池，在本节中我们需要对侯选池中的知识进行提炼以获得可信度更高的知识。4.3.1.词对排序本节采用排序加阈值筛选的方法而不是非黑即白的二分类方法，因为考虑到在某些特定的情境下不是那么符合常理的组合也能出现意想不到的效果，比如中的形容“薄薄的夜”，“薄”对于“夜”来说并不是一个常识性的属性，但是就艺术领域来讲却是一个能激起读者想象空间的绝妙形容。在第二节中我们针对知识提出的两种模式进行了定义，联结词会直接影响实体和属性的前后匹配关系，而实体属性的可信度也会映射到该联结词的其他匹配上。因此一个实体属性对如果能被多个不同的联结词联结，那么说明这个词对重要性很高，排名越靠前；同时排名靠前的词对会具备更大的表决权，即当一个联结词联结了排名高的词对，这个联结词的重要性也越高，关联模型结构如下图4.4所示：图4.4实体属性对和联结词之间的关系结构上图中E为实体，A为属性，con为联结词，X为实体属性对的序列，Y为联结词序列。要使得X,Y序列同时有序，这让我们想到了经典的二部图排序算法。由此我们统计EiconjAi的数量，得到一个h×r的矩阵：北京理工大学硕士学位论文32KN= 11 1r h1 hr其中h为实体属性对的总数，r则为联结词的数量。我们需要计算出一个h×1的矩阵SCEA和r×1的SCcon分别存放实体属性对和联结词的分数，两个向量结构如图4.5：图4.5两个结果向量结构示意图则利用迭代公式进行SCEA和SCcon的同时更新：SCconi=KN·SCEAiSCEAi+1=KNT·SCconij=1 SCconij × new_SCEAi+1=SCEAi+1j=1 SCEAi+1j × 其中 表示SCconi的维度，即该矩阵标准化后的总分，同样地 表示SCEAi+1的维度，标准化是为了将每个分数占比进行均衡后重新分配。最终，通过迭代运算至SCEA收敛即可得到实体属性对的得分，按照对应关系从大到小进行排序。因为实际计算中语料规模较大，经排列组合后的侯选池中实体属性对和联结词数量都较大导致计算过程中的矩阵较大，因此当SCEAi+1和SCEAi之间梯度误差小于10 7时判定为收敛停止计算。4.3.2.基于语义相似度的知识拓展以上的排序计算过程基础是语料中实体属性对和联结词出现的次数，但是必然存在的一个问题是一些虽然正确但出现次数少的因得分较低排名靠后，就比如“这座山峰从山脚下看起来很壮丽”，这个例子中“山峰-壮丽”这一词对的联结词是“从山脚下看起来很>”。这样的联结词出现概率必然是较小，这很可能导致“山峰-壮丽”和“从山脚下看起来很>”排名低。北京理工大学硕士学位论文33但是在对实体和属性的配对研究过程中发现：语义相近的实体情感属性可以共通比如“山峰”是“壮丽”的那么“高山”也可以“壮丽”，同理相似的属性也很可能可以修饰相同的实体，例如“广阔”用来形容“天空”推理出“辽阔”也可以。由此我们使用词语在文档集中的语义相似度进行可用知识扩充。计算语义相似度首先要进行文档集的词向量生成，我们借助word2vec模型对文档集进行无监督学习得到每个词表示向量，计算要进行匹配的词向量相似度完成知识扩展。4.4.实验分析4.4.1.实验参数设置本实验使用第三章第一小节中的表演创意文本数据集作为抽取语料，进行抽取之前同样采用数据过滤方法，然后进行以句子为单位的切分、分词、词性标注以及进行初步的侯选池构建，初步获得数据如下表4.3所示：表4.3侯选池规模句子切片词切片实体属性对联结词27856305512439813786841632679其中进行句子切点判断的正则匹配规则为“.|!|?|;|；||。|！|？|…|*|:|：”共13种字符。下表4.4展示了侯选池中排名前20的实体属性对：表4.4侯选池中排名前20的实体属性对排名实体属性1青松茂盛2白雾缥缈3石级陡4九龙壁有名5无锡有名6龙井著名7崂山著名8兰桂坊有名9中山陵著名北京理工大学硕士学位论文3410石梯险峻11胜迹著名12门槛低13山势陡峭14气派恢宏15佛门清净16风铃悦耳17凉风惬意18纳凉惬意19灵隐寺旺20雕工精细在对侯选池中知识排序结果进行初步调研时发现排在约35万名的结果开始出现较多错误。因此我们按照排序比例划分选择五组提炼比例进行对比试验，设置如表4.5所示：表4.5实验对照组设置保留10%20%30%50%70%提炼90%80%70%50%30%4.4.2.结果有效性分析实验中的准确率评价采用人工评价方式，选取五名具备大学及以上文化背景的被试者进行评价，同一对照组每人抽取三次，每次抽取50条知识，方差超过5.0则重新抽取评价样例进行再次评价，评价流程如下图4.6所示：北京理工大学硕士学位论文35图4.6准确率评价流程经以上流程评价得到五个对照实验组的准确率结果见下表4.6：表4.6不同比例提炼的准确率对比保留比例10%20%30%50%70%准确率95.7%91.3%82.1%48.9%23.9%从上表中首先可以证实排序中排名越靠前的知识正确率越高，说明排序算法的效果还是比较好的，其次，侯选池中从30%往后的准确率即使加上提炼步骤准确率也下降明显，因此在实际应用时应将保留比例定在前30%以内。4.4.3.知识库搭建经过上述步骤和研究分析之后本文选取了超过20万条知识作为创意情感资源，如何存储这样一个庞大的网状图谱是我们接下来要面对的问题，这时候传统的关系型数据库在查找关系技术性的延伸方面有极大的弊端，难以满足后续的生成联想要求，而原本就支持关系型数据的图数据库是最好的选择。图数据库，顾名思义是一种以图结构进行存储和查询的数据库，在本研究方法内的应用可以为整个框架带来性能的提升和成本的下降。组成图形数据库的元素主要分为两种，结点集和连接结点的关系集，关系是图数据库特有的结构。结点集和关系集这两种元素也刚好可以对应知识图谱架构中的实体/属性和关系，并且这种通过边将顶点连接在一起的结构能够快速地进行图北京理工大学硕士学位论文36检索操作。在此基础上，我们选择了近几年来流行度较广的Neo4j图数据库来构建最终的存储模块。Neo4j拥有强大的图形搜索能力和一定的横向扩展能力，还拥有面向网络的图形化管理界面，优势诸多。为了将原始提取的三元组形式数据导入其中并且成功构建出图的网式效果，需要先进行预处理，从总表变为一个结点表和一个关系映射表，格式如下图4.7所示：图4.7结点表的前十行展示关系表的前十行展示其中结点表中包含3个字段，分别是“node:ID”、“name”、“:LABEL”。“node:ID”表示结点的ID，结点指实体和属性两种，格式为ei其中i按照录入知识的顺序依次递增从而作为实体和属性的编号，不与其他结点重复；“name”表示实体/属性的名称；“:LABEL”则表示结点的标签，可以自行设定，如本项目中的“my_entity”、“my_attribute”。关系表中则包含4个字段：“:START_ID”、“:END_ID”、“name”、“:WEIGHT”，其中一行数据中的“:START_ID”和“:END_ID”分别表示某一条知识的实体和属性两值，也就是箭头的发出方和接收方，存储的字段是使用的结点表中该结点的“node:ID”号，因为“node:ID”具有唯一性，因此可以完成唯一映射；“name”表示此关系的名称，“:WEIGHT”表示该情感属性相对于此实体来说所占的权重。将形成的结点表和关系表写入Neo4j数据库中去，最终形成如下图4.8所示网状效果：北京理工大学硕士学位论文37图4.8知识库部分知识互联效果该知识库将用于后续创意生成时的关键步骤：面向实体的情感属性搜索，完成根据各实体基于情感属性关联的属性选择目的，实现情感要素融入。4.5.本章小结本章提出了一种从非结构化数据集中获取基于情感权重的创意知识的方法。在本方法中，我们首先通过词性和词间联结关系构建知识侯选池，并借用TF-IDF逆向思想研究实体属性对的权重分布，然后通过二部图排序算法获得词对的得分与排序进行可信度研究，再通过语义之间的相似度判别完成进一步知识拓展，最终将数据有组织地写入图数据库中完成创意资源知识库的构建。该方法的主要优势是：无需人工制定情感规则，可以无监督地基于大规模语料进行知识抽取。本章实验表明构建的基于情感权重的创意知识抽取方法能够有效地完成从大规模语料中自动抽取知识的任务，并且在权重分布和可信度研究的帮助下进一步完成对语料深度语义的研究和抽取。最后知识库的构建帮助数据更好地组织管理和可视化，也更方便后续的利用。北京理工大学硕士学位论文38第5章多条件约束的创意生成5.1.引言在整个研究过程中，我们将创意的主要生成过程总结为五个关键环节：素材收集，规则研究，情感赋予，逻辑联结，最终呈现。第三章和第四章分别对目前中国主流的创意体系和创意语料中的实体情感知识进行了研究与分析，为创意生成提供了规则和素材。本章旨在讨论如何将这些规则融入生成流程，如何选取知识才能组合出更优秀的创意。因此，我们提出了一种基于创意建模方法和知识库的主题词扩展方法，能够依据创意体系和情感知识库的约束进行实体与属性的搜索与扩展。同时还提出了一种基于co-attention的多约束创意文本生成方法，创造性地利用协同注意力机制完成无序词组到创意文本的有效生成，并基于生成的创意文本进行创意画面合成达到双模态创意展示的目的。本章研究结构如下图5.1所示：图5.1第五章研究架构图5.2.基于创意体系和知识库的主题词扩展创意创作的起始点往往可能是一个主题词，然后通过个人积累的常识库或加上一些辅助手段进行素材的收集或拓展。本在旨在根据前文已经获得的创意建模和知识库成果对起始主题词进行有效扩展。北京理工大学硕士学位论文395.2.1.面向规则的创意实体选择素材拓展的首要对象是实体，也就是根据主题将符合规则的相关实体进行罗列。这其中的规则有很多，例如创意实体的前后相关性，实体排布的逻辑性等。就比如常识性认知中，小草和大树的相关性就显得比小草和大海的相关性要更高，而一个主题是梅花的表演中出现过多的牡丹也显得喧宾夺主。这就要求我们在选择实体上能够遵循这些深层的实体语义规则，传统的创作流程依靠创作人的艺术素养进行规则约束，但必然存在过于依赖个人审美且创作人自身积累过程慢等缺陷。随着人工智能技术的发展，也涌现出很多基于人工智能算法的创意生成技术，但是目前的技术主要是基于已有的数据和模式来生成新的创意，又因为机器缺乏人类的审美和判断力，很容易出现低审美或低实用性的创意，这同样是生成过程中对于细小规则约束力弱的原因。在第三章中我们通过收集到的中国表演创意文本数据集对国内优秀创意开展研究，利用LDA文本建模方法进行了面向文本资源的主题和词的分布建模。在这个过程中，LDA计算出每个主题中的单词分布，以及每个文本中主题的分布，将文本到主题再到词语一层一层联系起来，这些分布和联系中蕴含创意的构建规则。具体来说，可以利用LDA得到每个主题中高频单词，从而确定该主题的核心概念和主题方向；同时，根据每个主题中词语的概率分布，可以确定组织该类主题时需要如何分配实体类词语。也就代表着LDA模型可以帮助某主题随机生成一组由N个单词组成的集合。首先要生成单词组合要有一个采样库，也就是词料库，这同样经第三章的建模实验已有成果，经重新组织如下图5.2：图5.2LDA模型词料库北京理工大学硕士学位论文40每列为一个主题，每个主题中有众多与此相主题相关的词。词料库中单词为多项分布，词料库本身主要由两个参数进行描述，也就是LDA模型中的超参数 和 。即主题在文档中出现的概率 符合Dirichlet分布，而主题i条件下生成第j个词的概率为 ij。因此得到 和 之后就可以进行词组提取了：假设需要进行提取的长度为L，对于L个单词中的每个词则有：•选择主题zn，zn服从multi分布；•根据概率 ij选择所需单词wn.基于以上分析，利用LDA模型进行实体扩展的基本算法如下：算法5-1：基于LDA模型的实体扩展算法已知： K×1， K×V， ，主题词W输入：词组长度L；过程：1：由 ~Dirichlet得 1, 2,…， K;2：计算主题词W在各主体上的分布概率得到pW=;3：forn=1:Ldo4：选择此次主题zn5：依据pW计算需要在主题zn中选择的词数量N6：forj=1:Ndo7：词wi出现在主题zn中的概率为p8：进行一次词采样9：endfor10：endfor输出：词组SL在给定了L之后，上述过程实际上是一个采样的过程，也就是所有的步骤都是随机的，但是这样的随机过程必须服从相应的分布，而这些分布的参数从基于中国表演创意文本数据集的主题建模中来，就是我们需要依循的创意规则。北京理工大学硕士学位论文415.2.2.基于知识关联的情感属性搜索由于机器难以考虑到人类的情感认知和审美，故虽智能化方法可以提供一些有趣和新颖的创意，但它们可能缺乏与人类情感和主观体验相关的因素，使得这些创意缺乏情感共鸣和人性化的特点。因此，在智能化手段生成创意的过程中，需要考虑如何加入情感和主观体验因素，从而使得生成的创意更具人性化特点。本文在第四章节中提出了基于情感权重的创意资源知识库构建方法，利用大规模文档资源中形容词对实体词的天然修饰进行非结构化数据的情感知识抽取，并利用知识的关联性构建创意资源实体与属性的图谱结构。本节的主要任务旨在利用创意资源知识库对上节中依据主题模型约束得到的实体词组SL进行属性扩展。接下来需要面对的问题是如何在众多属性中进行选择才能够保持情感上的一致性和关联性。经观察发现，库中的知识资源由实体、情感属性和加权关系三类元素组成，基于实体与属性之间的互联关系，总结出知识在结构上的两种特性：发散性和传递性。如下图5.3所示：图5.3发散性结构传递性结构这两种结构特性有利于进行属性扩展，其中发散性能够提升创意的多样性，避免每次扩展都是同一属性导致千篇一律，丧失最重要的创新性。传递性则能够对前后实体的情感进行进一步约束，因为我们认为同一属性的实体在概率上有更大可能具备情感一致性，且这个一致性是可传递的。基于以上原则，研究问题转化为寻找实体结点间的连接路径，而知识库的组织形式是天然的图结构，因此借助路径规划算法的思想进行属性偏向查找，那么主要的步骤转化为：•已知我们最初输入的主题词W作为起始点；•实体词组SL中抽取需要进行属性扩展的实体点Ei作为终点；北京理工大学硕士学位论文42•寻找W到Ei的最短路径；•如有多条可达，则依据Ei的属性权值分布选取权重大的属性；•记录选择路径上未被录用的其他属性加入备选属性集合As；•重复以上步骤直到完成待修饰实体的属性扩展。在针对单个实体点Ei进行W到Ei的属性偏向路径查找时，根据知识库中的实际分布和查找结果，分为以下几种情况：①单路径到达如下图5.4中的A到G，虽说也有多条路径到达，但这其中最短路径只有“A-B-G”，此时就选择B作为实体G的属性扩展；图5.4单路径到达示意图②多路径到达如下图5.5中的A到N，共有5条等长路径可达到：“A-B-G-O-N”、“A-C-F-L-N”、“A-D-J-L-N”、“A-H-J-L-N”、“A-H-I-M-N”，几条路径A到N之间均需要经过3个结点，此时我们看N周围的三个属性O、L、M占据实体N的情感比重分别为：0.4、0.3、0.3，也就是说对于N来说倾向于O属性的概率更大，因此选择O作为实体N的属性扩展。；图5.5多路径到达示意图北京理工大学硕士学位论文43③不可到达不可到达又分为两种情况，一种是真不可到达，也就是两个实体所在的子图是不连通的，如下图5.6的A和P，此时无路径可使A、P连结；图5.6真不可到达示意图另一情况是定义不可到达，也就是两个实体之间虽然连通但是中间节点数太多，如下图5.7的A和S，这种情况下虽然有路径可使两实体连通，但中间结点数量超过15，此时路径上的一致性传递逐渐减弱，我们认为不再满足传递规则约束，因此定义为不可到达。图5.7定义不可到达示意图在不可到达的两种情况中，我们选择使用词向量相似度计算的方法将P/S具有的属性和备选属性集合As重点属性进行匹配，保留匹配度高的属性。5.3.基于co-attention的多约束创意剧本生成经过以上步骤目前已经从主题词扩展成一些关键词的堆叠，也就是一个基于规则主题和情感约束的词组，但是词组首先是无顺序的，其次从语法上看很可能是不通顺的。这也是必然的因为在进行创意建模和知识库构建的时候并未考虑词间的联结逻辑关系，而是依靠词袋模型进行。因此这一节的主要任务是通过文本处理的智能方法进北京理工大学硕士学位论文44行基于词组的多约束条件下的创意文本生成。常规的文本生成任务对生成文本的内容通常没有强制性的约束，而受控文本生成任务会要求生成文本的内容必须满足一些既定的约束条件，如风格、主题等。本节将受控文本生成应用到创意文本生成任务上，该任务的输入为给定的词集，计算机需要自动生成一个能够基于词集且逻辑通顺的文本。例如，给定一个概念集合，理想情况下机器应该生成“男人去了山上的凉亭”这样的句子。如果机器生成了“男人在凉亭里”或者“男人爬了凉亭里的山”，都不能算是一个合格的生成结果。因此，要想获得一个较好的生成结果，我们需要保证词集里的实体和属性元素尽可能都出现在输出结果中，并且符合常识和语法逻辑。5.3.1.数据预处理数据预处理主要完成从文本到表示向量的映射，以便于神经网络的处理。本节的方法不能再是仅通过词频统计这样简单的方式进行文本特征映射，而是需要涵盖更丰富也更复杂的语义信息和上下文相关表示。使用词嵌入技术可以提高模型处理任务的准确性，使计算机更好地理解和处理文本数据。因此我们选择BERT模型进行词向量的生成。BERT是一种预训练的深度学习模型，可以用于生成高质量的词嵌入向量。BERT的网络架构采用了多层Transformer结构，使用BERT进行相关文本处理任务的优势是BERT表征会基于所有层中的左右两侧语境，这与后续模型中的生成需求是相符的。数据处理过程如下图5.8所示：图5.8数据处理过程示意图针对BERT模型目前已经有很多学者提供各种规模和性能的的预训练模型，首先我们使用基于tensorflow的chinese_L-12_H-768_A-12作为初始预训练模型。其次，是对输入模型数据的初步处理，根据模型输入需求要将我们收集到的文本语料进行分北京理工大学硕士学位论文45词添加标记等处理。BERT模型处理之后根据后续具体任务能够输出词向量、位置向量和段向量。我们选择第5层和第11层的平均值作为最终的词向量。5.3.2.模型设计基于多约束条件的生成需求，本节提出一种基于双向循环神经网络结合co-attentation的方法，以BRNN作为基础模型，中间添加co-attention层将词组的信息与生成文本的信息进行交互从而达到更准确地在约束条件下生成文本的效果。BRNN是两个RNN进行叠加的模型，而RNN是经典的序列处理模型，可以依据之前时刻的时序信息来预测下一时刻的输出，但标准的RNN在时序上处理序列，往往忽略了未来的上下文信息。BRNN很好地解决了这个问题，利用两个RNN上下叠加在一起，那么当前时刻的输出不仅和之前的状态有关，还可能和未来的状态有关系，也就是同一层节点之间的信息是双向流动的。基本思想是使每一个训练序列的正向和反向形成两个RNN，而且都连接着一个输出层。该结构可以为输出层提供输入序列中每个点的双向上下文信息，原理如下图5.9所示：图5.9BRNN原理架构图而在模型设计中添加co-attention层是关键步骤，该层将词组的信息与文本的信息进行交互，以产生更好的文本生成效果。在这个模型中，co-attention层的作用是通过将词组和文本的信息进行交互来增强文本生成的效果，因为词组往往可以提供额外的上下文信息，帮助模型更好地理解输入的文本。因此通过这种方式，co-attention可以帮助模型更好地理解输入的词组，可以学习到文本和词组之间的相似性，并将该相似性作为权重对文本进行加权平均，同时也会对词组进行加权平均操作，从而生成更加北京理工大学硕士学位论文46准确的文本。模型架构如下图5.10所示：图5.10BRNN+co-attention模型架构图其中，经扩展的词组V作为原始输入进入模型，经过词嵌入将词转化为词向量，输入包含多个词向量将词向量拼接形成大小为L×d的矩阵X，其中L表示输入序列的长度，d表示词向量的维度。此外设置一个查询向量q，维度为d，表示模型当前生成的上下文信息。在生成每个词时，将q和X输入到co-attention层中，以获取q和输入词之间的关联约束信息。通过计算q和X中每个词向量之间的相似度，得到一个L维向量 ，表示每个输入词在关注查询q时的重要性。然后，将 和X做加权平均，得到一个加权后的向量z，表示输入词的重要信息：z=i=1L iXi 将z和q拼接在一起，得到一个d*2的矩阵Z，作为co-attention层的输出。将Z输入到后续的生成模型中，可以用于生成下一个词。需要注意的是，上述过程在每个时间步上独立进行，以便生成整个文本序列。同时，为了更好地利用上下文信息，将前面生成的词向量进行拼接，形成一个上下文矩阵，作为查询向量q的一部分，以便更好地捕捉上下文信息的影响。北京理工大学硕士学位论文475.4.创意画面合成创意可视化指将创意通过视觉表现形式呈现出来的方法。这种方法能更直观地被展示创意，更好地辅助创意思想的传达。创意画面生成帮助创意脚本在其他模态下进行多视角展示，帮助创作者更全面地理解和评估创意。此外，创意可视化还可以帮助设计师更好地表达他们的设计思路，从而让客户更好地理解他们的设计理念。特别是在以上步骤中，我们已经获得了文本创意的基础之上，可视化能够很好地帮助我们自身直观的看到创意成果，同时也能够从另一角度评估创意生成的成功性。传统的创意可视化过程是通过绘图、制作或直接人力排演呈现等方式进行，过程需要较长的时间和高昂的费用，而且需要专业技能。且人类创意的输出也受限于个体的经验和能力，难以实现大规模、高效率的生产。智能化手段能够高效地学习画面构成、风格等要素进行创意画面合成，目前许多深度学习模型和人工智能技术都在这方面取得了不错的效果。本节通过对扩散模型进行中国创意画面方向的微调从而达到更好的以中国创意脚本为基础的创意生成。在扩散模型出现之前，计算机视觉和机器学习方面最重要的突破是生成对抗网络GAN。GAN让模仿和超越训练数据内容成为可能，从而打开了一个全新领域——生成建模。然而，在经历了一段蓬勃发展后，GAN开始暴露出一些瓶颈和弊病，众多研究也鲜有突破，例如：图像生成缺乏多样性、多模态分布学习困难、训练时间长等。近几年，随着算力的增长，一些过去算力无法满足的复杂算法得以实现，其中有一种方法叫“扩散模型”——一种从气体扩散的物理过程中汲取灵感并试图在多个科学领域模拟相同现象的方法，该模型在图像生成领域展现了巨大的潜力。扩散模型是一种生成模型，用于生成与训练数据相似的数据。简单的说，扩散模型的工作方式是通过迭代添加高斯噪声来“破坏”训练数据，然后学习如何消除噪声来恢复数据。一个标准扩散模型有两个主要过程：正向扩散和反向扩散。在正向扩散阶段，通过逐渐引入噪声来破坏图像，直到图像变成完全随机的噪声。在反向扩散阶段，使用一系列马尔可夫链逐步去除预测噪声，从高斯噪声中恢复数据，如下图5.11：图5.11正向反向马尔可夫链北京理工大学硕士学位论文48扩散模型通过从正态分布变量中逐步去除噪声来学习数据分布。换句话说，扩散模型使用长度为T的反向马尔可夫链。这也意味着扩散模型可以建模为时间步长为t=1,2,……,T的一系列“T”去噪自动编码器。模型损失函数定义为：LDM=Ex~N,t在文本到图像的合成中，潜在扩散模型使用预训练的CLIP模型3，该模型为文本和图像等多种模态提供基于Transformer的通用嵌入。然后将Transformer模型的输出输入到称为“diffusers”的潜在扩散模型PythonAPI，同时还可以进行扩散步数、随机数种子、图像大小等参数的调整。深度学习的训练，首先就是要解决数据问题。由于stablediffusion的训练数据是文本-图像匹配的pairs，因此我们要按照它的要求准备数据。根据前文对于中国创意文本的主题建模，我们从“情节、历史、文化、艺术、时代”几个方向进行个人图集的收集，部分图集如下图5.12所示：图5.12图集数据展示准备好所有图片之后是进行文本描述配对，以为内目前收集的图片数据都是没有文本标注的，此处使用BLIP算法来自动生成标注。完成以上准备任务之后，设置学习率、训练batch大小进行微调训练。5.5.实验分析5.5.1.实验环境本章实验主要验证从实体词到词组扩展、词组到创意文本生成以及创意可视化三北京理工大学硕士学位论文49个阶段的效果来综合评判本文中创意生成方案的有效性。实验均在实验室分服务器上进行，配置如下表5.1：表5.1实验环境配置设备名版本型号处理器IntelCorei9-10900KFCPU@370GHz3.70GHzGPUNVIDIAGEFORCERTX30803080×4内存64GB操作系统Ubuntu18.04IDEanaconda3-4.12.0CUDA10.15.5.2.生成效果展示创意词组及文本生成效果对比本文采用不同数量扩展词进行词组及创意文本生成操作的对比展示：1）第一次对比实验：选取主题词“雪花”分别进行个数为“3”、“5”、“8”的实体扩展以及后续的生成。主题词雪花前3词扩展实体选择景观、地面、树枝属性搜索景观-美妙、地面-潮湿、树枝-曲折生成文本在一个寒冷的冬日早晨，站在窗前欣赏着美妙的雪景，雪花飘落在曲折的树枝上，把整个景观覆盖了一层白色的薄雾，仿佛置身于一个童话世界。地面潮湿的感觉让人感到温暖，仿佛所有的一切都在为这美丽的景色而欢呼。这一幕仿佛是一个迷人的艺术作品，让人沉浸在纯净和美妙的氛围中，直到第一缕阳光洒落在这片雪景上，为整个世界带来新的生命。前5词扩展实体选择景观、地面、树枝、打雪仗、雪人属性搜索景观-美妙、地面-潮湿、树枝-曲折、打雪仗、雪人-可爱生成文本在一个雪花纷飞的冬日，小男孩和朋友们来到了一片美妙的景观北京理工大学硕士学位论文50中，树枝曲折，地面潮湿，仿佛整个世界都被雪花覆盖了一层白色的薄雾。他们在这里打起了雪仗，追逐着对方，还一起制作了一个可爱的雪人，给这个美妙的景观增添了更多的乐趣和欢笑。雪人的身体由大大小小的雪球堆叠而成，树枝扭曲着成为了雪人的手臂，雪花飘落在它身上，仿佛为它穿上了一件纯白的外衣。整个雪人看起来十分可爱，让人不禁想要拥抱它一下。前8词扩展实体选择景观、地面、树枝、打雪仗、雪人、装饰、家庭、新年属性搜索景观-美妙、地面-潮湿、树枝-曲折、打雪仗、雪人-可爱、装饰-有趣、家庭-温暖、新年-热闹生成文本在一个雪花飘飘的冬日，男孩和家人来到了一处美妙的景观中。地面潮湿，树枝曲折，白色的雪花铺满了整个大地。他们在这里打起了雪仗，笑声和欢呼声在空气中回荡，仿佛所有的烦恼都被抛到了脑后。接着，还一起创作了一个可爱的雪人。这个雪人不仅仅是一个普通的雪人，而是被精心装饰成了一个有趣的角色。它有一双搞笑的大眼睛，一个胡萝卜做的鼻子，仿佛一个小小的新年吉祥物。这个有趣的雪人成为了家庭温暖的象征。夜幕降临，热茶、烤棉花糖和新年小吃让这个夜更加亲密和热闹。2）第二次对比实验：选取主题词“立春”分别进行个数为“3”、“5”、“8”的实体扩展以及后续的生成。主题词立春前3词扩展实体选择正月、节气、春游属性搜索正月、节气-二十四、春游-美好生成文本在立春的那一天，正月的喧嚣渐渐消散，人们开始期待春天的到来。二十四节气中，春分、清明、谷雨等都是与春天密切相关的。而春游更是一个美好的活动，让人们在春天里感受大自然的生机勃勃。前5词扩展实体选择正月、节气、春游、迎春、播种属性搜索正月、节气-二十四、春游-美好、迎春-绚丽、播种-辛勤生成文本随着立春的到来，正月的气息逐渐消散，大自然也开始进入一个北京理工大学硕士学位论文51新的循环。在中国传统文化中，二十四节气是人们记录季节变迁的重要方式。在这个美好的春天里，我们可以一起春游，欣赏樱花、品尝春茶，感受迎春的绚丽和大自然的馈赠，也可以体验播种农作物的辛勤和快乐。前8词扩展实体选择正月、节气、春游、迎春、播种、春卷、天气、踏青属性搜索正月、节气-二十四、春游-美好、迎春-绚丽、播种-辛勤、春卷-美味、天气-明媚、踏青生成文本随着立春的到来，正月的喧嚣慢慢平息，大自然也开始迎来一个新的季节。在中国传统文化中，二十四节气是人们记录季节变迁的重要方式，春分、清明、谷雨等节气标志着春天的到来。春天是一个美好的季节，春游是让人们放松身心的好方式。在这种明媚的天气里，我们可以一边欣赏春景，一边品尝美味的春卷，感受迎春的绚丽和播种的辛勤。我们还可以去郊外踏青，欣赏樱花、桃花等美景，感受春天的美好。让我们在这个美好的春天里，一起迎春、播种、春游，共度一个美好的时光。创意画面合成效果对比本文采用生成的创意文本经本文算法合成画面和必应搜索引擎搜索出的图片进行对比，对比效果如下图5.13和图5.14所示：1）主题词：雪花图5.13围绕“雪花”生成的创意文本可视化对比北京理工大学硕士学位论文522）主题词：立春图5.14围绕“立春”生成的创意文本可视化对比通过对两个主题进行搜索引擎和创意图像合成的效果对比可以看出，本文最终生成的效果在画面内容构成要素方面更加丰富，特别是人物的互动性更强，在与文字的契合度上也更高，极少出现像是搜索结果中的噪声信息。5.5.3.实验评价指标实验共分两步：①主题词创意文本的生成；②创意文本画面合成。以上分段进行详细的实验验证与分析。主题词到创意文本的生成目前还没有一个完美的评价指标用来评价创意文本的生成，为了尽可能用更全面的角度对生成结果进行评价，我们采用主观与客观相结合的方式进行，客观角度包括使用词组覆盖率、以及BLEU和METEOR两种自动评估方法来对生成结果进行评估。主观评估方式采用系统功能试用和问卷调查的方式。1）BLEU评估指标BLEU是一种评估自然语言生成模型输出质量的指标，通过计算生成文本与参考文本之间的n-gram重叠度来度量生成文本的质量。因此该方法采用n-gram算法计算生成句子和评估句子之间词重叠率给出评分。BLEU指标的计算公式为： = =1 × 北京理工大学硕士学位论文53 =1 > exp1 其中c表示生成文本的长度，r表示最短的参考文本长度，BP是惩罚因子，用来惩罚生成文本果断的情况。 指当前单元词组的权重， 代表准确率。2）词组覆盖率词组覆盖率指标 主要评估从词组到创意文本生成实验中是否能够较好地覆盖之前的实体和属性扩展成果。3）METEOR评估指标METEOR方法常用于机器翻译，它同时考虑精确率和召回率，通过生成文本与参考文本之间的chunk进而评价句子的流畅性，并且采用加权的F值来作为评估指标。METEOR指标的计算公式为： = · · +1  = · 其中P和R代表准确率和召回率， 为超参数， 是基于生成句子流畅性的考核加入的一个惩罚参数， 和 都为超参数，frag=c /m，ch表示根据句子对齐文本的分块数，m为句子中词汇总数。最后得出分数： =1 · 4）人工评价指标以创意生成成功率和三次推荐满意度两个角度为基础进行面向系统功能性测试的问卷调查，其中创意生成成功率衡量被试者是否认为生成结果满足创意这一标准，而三次推荐满意度指标衡量在多次推荐的情况下是否至少有一次推荐内容能够符合被试者创意需求。为了降低个人评分尺度对评价结果的影响，使用李克特七级量表来测度平台使用者的态度或者看法。即将体验感分为七个等级：极其符合、符合、绝大部分符合、大部分符合、基本符合、局部较差、不符合。用户根据自己的体验进行选择。创意文本画面合成创意画面合成模型本身在生成图像的合理性上是已经验证的，因此本文主要从：契合度、创意性两个角度进行有效性分析。1）契合度北京理工大学硕士学位论文54用以衡量合成的图像和对应文本描述之间的视觉-语义相似度，也即图像与创意文本的契合性。通过对图像中出现的实体数统计进行衡量，如果可以覆盖60%以上的实体即为相关。2）创意性创意性是较为主观的评价标准，但是在创意生成过程中又是不可缺少的评价指标，且创意性的评价需要评价者本身对创意深入了解，具备较高艺术素养。对于这项指标，我们采取专家打分的方式进行结果采集。5.5.4.结果分析本文实验在模型上设置对照组以验证co-attention层的有效性，同时设置扩展词个数进行模型生成效果在不同长度词组上的研究，如下图5.15：图5.15实验效果对比总体来说，co-attention层可以使得模型在词组到创意文本的生成任务上表现更好，且性能提高程度还是较为明显的。而在不同实体扩展长度的实验对比中可以看到无论有无co-attention层，生成效果都会在扩展长度越长时呈现大致的下降趋势，这也是与常识相符合的，模型在兼顾较多实体时会导致一定程度上的流畅度下降。因部分研究表明BLEU更适用于评估短文本生成任务，而不适用于长文本场景中，原因在于它不能很好的评价上下文关联。加上我们对于本文框架的定义需要生成的创意文本在一定程度上遵循前面的实体与属性扩展工作，特别是长文生成上，因此我们还引入了覆盖率 和meteor指标进行对比评估，如下图5.16：北京理工大学硕士学位论文55图5.16不同扩展词数覆盖率 和meteor指标对比经上述实验分析对比，实体扩展在3~5个是较为合适的长度，当超过5时在meteor指标上呈现明显的生成效果下降。主观评价是采用功能使用以及问卷调查的方式进行结果统计与评估，我们将创意生成功能嵌入“大型表演活动数字化策划与智能创意平台”中，如图5.17所示，并召集不同年龄层级，不同职业，不同专业背景的志愿者进行系统网页访问和随后的问卷填写。图5.17功能嵌入界面展示在问卷中主要对创意生成成功率和三次推荐满意度两个评价指标进行调查，为了使得志愿者对这两个评价的指标更为具体，让评价者更易于理解，从而降低随机因素对评价结果的影响，我们对两个评价维度进行评价问题的细分，具体细分问题如下表5.2所示：北京理工大学硕士学位论文56表5.2问卷内容设置评价维度评价问题创意生成成功A.生成的三项结果不重复B.生成结果长度合适C.生成结果语句通顺表达清晰，符合文本语法D.生成结果切合输入约束E.生成结果是新的而非照搬现有剧本三次推荐满意F.生成的结果中至少一条能够体现中国文化表演要素G.生成结果中至少一条具备创意文本所需形象或情节H.生成结果中至少一条符合预期或能够产生启发针对每个问题设置的七个评测等级对应的分数分别为：100、90、80、70、60、50、40。通过分数计算达到对创意生成成功、三次推荐满意这两个维度的评估。每份评卷对两个指标得分的计算方式如下： 1=A 20%+B 20%+C 20%+D 20%+E 20% 2=F 30%+G 30%+H 40%其中 1指一位志愿者针对创意生成是否成功进行的评分， 2指一位志愿者对三次推荐内容是否满意进行的评分。因60分对应等级“基本符合”，也切合大多数人60分及格的日常认知，因此我们对标准略作提升，两个分值设置达到70分为成功/满意。最后问卷系统共收集到517份有效提交，并经统计，结果如下图5.18，判定为成功/满意的问卷数量得出创意生成成功率和三次推荐满意度分别为：87.62%和85.88%。图5.18问卷调查结果统计北京理工大学硕士学位论文57除此之外对收集的问卷中两个指标的分布情况进行阶段性统计，如下图5.19：图5.19评价数据分阶段统计可以看出两个指标的打分分数集中在70-90分区域，而就两个指标横向对比情况来看三次推荐满意度分数整体偏向更高。这说明大家对于创意生成成功的标准要求较高，但是三次连续推荐的内容中还是有很大概率能满足大部分人的创意需求。创意画面合成和创意文本是相辅相成的，而又因创意画面主要根据创意文本生成，因此在已有对创意文本的衡量结果之后，首先衡量的是画面与文本的契合度，根据不同实体扩展数条件设置，统计覆盖率，计算方式参考图5.20：图5.20阶梯分布率统计最终对生成结果的覆盖率进行分阶段统计，结果如下图5.21：图5.21创意图像实体覆盖率统计结果北京理工大学硕士学位论文58由指标文本图像契合度定义知覆盖实体60%以上的为相关，因此可得此次实验中图文相关指数达到73.02%，能够较好地满足创意画面合成需求，辅助创意展示。我们经过一系列的步骤从主题词得到最终创意合成画面，创意性的评估同时也一定程度上是对整个方法创意性的评估，创意性我们采用专家评级的方式进行，创意性等级及描述如下表5.3所示：表5.3创意性评判等级及对应描述创意性等级等级描述5该等级图片具有非常独特的理念或概念，或具有非常新颖的方式，或具有新颖的方式，能够深深地激发创造力和想象力。4该等级图片具有某种独特的理念或概念，或具有新颖的方式，可以让人产生深刻的思考和感受，能够激发创造力和想象力。3该等级图片可能有一些独特的理念或概念，或具有新颖的方式，可以激发人的想象力和创造力。2该等级图片可能有一些构思、角度、或颜色，但是难以激发人的想象力或创造力。1该等级图片非常普通和常见，缺乏独特性和创造性，没有什么特别之处，几乎不会引起人的兴趣。选取50幅最终合成的创意图像，并邀请专家进行创意性评级，最终统计平均得分为3.74分，也就是在“中等”和“高”之间，也就是说本文方法得到的结果在一定程度上具有一些独特的理念或概念，或新颖的方式，可以激发人的想象力和创造力。5.6.本章小结本章提出了一种从主题词到创意的生成方法。在本方法中，我们通过前文对中国创意表演体系的主题建模进行基于概率分布的规则化实体选择，利用构建的创意资源知识库查找扩展实体的属性修饰，并借用图数据库的知识关联结构和情感权重完成属性确定。在获得了一系列的相关词组之后，利用基于co-attention模型的多约束创意生成模型完成从词组到创意文本的生成并通过finetune扩散模型完成面向表演创意的画面合成。该方法的主要优势是：完成一套从主题词到最终双模态展示创意的全流程框架建设，同时生成步骤的划分摆脱了以往生成模型的“黑盒”模式，让每一步都有规北京理工大学硕士学位论文59则可依。本章实验从主客观两个角度共六个指标上证明该创意生成框架是行之有效的，并且创意的结果从文本和图像可视化两个方向均为创作人员提供有效的展示方式和辅助手段。北京理工大学硕士学位论文60结论创意是表演中至关重要的元素，优秀创意的力量在于打破传统框架和想法并引起更多人的情感共鸣，为观众呈现出与众不同、引人注目的表演效果。本文聚焦于智能化表演创意生成方法，首先对优秀的创意承载语料进行面向表演的主题建模，研究了创意实体分布规则；接着提出了基于大规模非结构化数据的情感权重知识提取方法，构建创意资源知识库；最终利用实体分布规则和知识关联特性进行智能化创意生成，并在主客观评价指标中都取得了很好的效果。本文的创新点主要体现在四个方面：提出了智能化创意生成框架。本文聚焦于创意生成全过程，创新性地提出了从主题词到最终创意展示的表演创意生成框架，实现创意流程的有序激发，并在重点研发计划项目框架下完成系统功能集成、实验验证以及用户体验。进行了中国表演创意建模。本文首次提出并构建了中国表演元素为基础的文本数据集，并基于此数据集进行了面向中国表演创意的主题建模研究，为智能化创意生成中的元素堆叠提供规则依据。搭建了基于情感权重的创意资源知识库。基于融合情感要素的创意生成需求，本文创造性地提出了情感权重知识抽取方法，从大量非结构化文本中进行实体情感属性知识抽取，并通过实体-属性在语料中的多层次分布计算每个实体的情感属性权重，最终构建基于图数据库的创意资源知识库，实现创意资源的高效管理。提出了基于co-attention的多条件约束创意生成方法。在经过基于规则和情感知识的实体、属性选择之后，需要完成基于词组的多约束创意生成，本文利用BRNN模型进行文本双向信息提取并利用添加的co-attention层进行词组和目前生成文本间的信息交互，完成覆盖词组的文本生成。并通过后续的创意画面合成实现创意可视化。本文从四个类别入手构建了一个约11.4G的中国表演创意文本数据集，提取了超过20万条情感知识并录入图数据库提供对外访问。本文以北京冬奥创意会主题为关键词进行实验生成文本和图像创意，并通过主观和客观双重评价指标对提出的方法进行性能和功能的评价，实验验证了本文方法在计算机辅助智能化创意生成方面的有效性和可用性，当然本文的研究方法仍然可以继续进一步的探讨：文本数据集的完善性。目前本文对于数据集的构建是在调查研究以及相关导演、艺术工作者的指导下进行文本的收集，能够保证一定程度上的要素完备性。但有关创意特别是好的创意脚本等文字性电子资源还是占比较少，下一步工作考虑对实物北京理工大学硕士学位论文61型文字资料进行更广泛的收集与录入，实现进一步的数据填充。创意知识的多模态扩展。目前本文在从文本型数据中进行知识抽取的流程被证实为是行之有效的，下一步可考虑以此作为基础进行图像、音频、视频等多种模态语义的知识对齐与扩展，实现更丰富的创意要素辅助。实现创意动态可视化。目前本文主要实现了在文本、图像两种模态下的创意生成效果。但对于一个表演来说，创意图像缺少时序性的动态变化，下一步考虑将时序因素融入创意的可视化，为创意选择、创意评估都提供更丰富的效果素材。北京理工大学硕士学位论文69致谢时间走的很快，我把致谢留到最后来写，想象了无数次写到这里的心情，才发现面对一个阶段的结束没我想的那么意到笔随。首先借此机会，请允许我向硕士生涯中所有指导、关心、帮助过我的老师、同学、朋友、家人们献上我最诚挚的谢意。首先感谢我的导师丁刚毅教授，丁老师拥有极为严谨的学术态度，总是在细节方面潜移默化地影响着我们的学业和研究，让我在专业上更加精进，受益良多。同时，我要感谢张龙飞老师，张老师不仅在论文撰写和发表上给予我细心严谨的指导和帮助，更是在我整个研究生阶段都对我的学习和工作生活提供倾尽全力的帮助以及无微不至的关心，感谢张老师一直以来不厌其烦的解答和细致耐心的指导。然后，我要感谢我的家人给予的关心和支持，感谢总能在我情绪最高涨或最低潮时及时拉我谈心，言传身教地告诉我解决办法应该放在情绪释放之前的爸爸；感谢听我乱七八糟分享，让我27岁还能回家说一声明天想吃红烧排骨的妈妈；感谢我的开心果、暖宝宝小弟。家人让我明白有个地方永远是我的退路。最后我想感谢自己，本科毕业并在工作了将近两年之后再次考研，听过最多的一个问题就是“为什么要读研？”，我想我用三年的时间回答这个问题。只有在真正经历之后才能明白这段时光有多珍贵。感谢母校接纳我，给我机会重新接受精神的成长和思想的蜕变；感谢实验室所有老师温和的态度、倾心的帮助让我领略人格魅力之妙；感谢我的同门、师兄弟、师姐，是他们让我体会到家人之外的亲情、友情，感谢他们提供的无私帮助以及可贵的情绪价值。我想一个阶段到另一个阶段的过渡总是鸡飞狗跳的，虽然这个过程也已有过不少次类似的经历了，但是作为一名硕士进入人生的新阶段还是那么的新奇。愿我能永远保持我的赤子之心，愿我能永远被人称赞“你父母把你教的很好”，愿我能变得更加通透，愿我能朝着勇敢自由之路一直走下去。"'''

# determine chunks
chunker = SentenceChunker(chunk_size=32, delim=[".", "!", "?", "\n", '。', '；', '！', '？'])
chunks = chunker.chunk(input_text)
chunks = [chunk.text for chunk in chunks]
# print('Chunks:\n" ' + '\n"'.join(chunks) + '"')
# encode chunks


# model_name = 'jinaai/jina-embeddings-v3'
# # load model and tokenizer
# model = AutoModel.from_pretrained(model_name, trust_remote_code=True, device_map=device)
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# embeddings_traditional_chunking = model.encode(chunks)

# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)



model_name = "BAAI/bge-m3"
# Initialize the SentenceTransformer model
from sentence_transformers import SentenceTransformer
model = SentenceTransformer(model_name, device=device, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# Encode each chunk and calculate the embeddings (returned as a NumPy array)
embeddings_traditional_chunking = model.encode(chunks, convert_to_numpy=True)


chunk_spans = []
for text in chunks:
    tokens = tokenizer.encode_plus(
        text, return_offsets_mapping=True, add_special_tokens=False
    )
    token_offsets = tokens.offset_mapping
    # tokenizer.decode(tokens.encodings[0].ids[0:5])
    token_cnt = len(token_offsets)
    chunk_spans.append([0, token_cnt])

embeddings, chunk_spans = _dynamic_chunking(embeddings_traditional_chunking, chunk_spans)
print("********************* Dynamic Chunker *****************")
for start, end in chunk_spans:
    print(f"Span: {start} - {end}")
    # Use end+1 in slicing since 'end' is inclusive
    print(" ".join(chunks[start:end+1]))

print("********************* Semantic Chunker *****************")
from chonkie import SentenceTransformerEmbeddings, LateChunker
from sentence_transformers import SentenceTransformer
# _model = SentenceTransformer(
#     "jinaai/jina-embeddings-v3",
#     trust_remote_code=True,
#     model_kwargs={"torch_dtype": torch.float16},  # or torch.float32
#     device="cuda" if torch.cuda.is_available() else "cpu",
# )

# model = SentenceTransformerEmbeddings(_model)
model = SiliconFlowEmbeddings()
chunker = SemanticChunker(
    embedding_model=model,
    chunk_size=128,
    threshold="auto",
    delim=[".", "!", "?", "\n", '。', '；', '！', '？'],
)
chunks = chunker.chunk(input_text)
for chunk in chunks:
    print(chunk.text)
    # Use end+1 in slicing since 'end' is inclusive
    print()